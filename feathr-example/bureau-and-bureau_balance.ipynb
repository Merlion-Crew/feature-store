{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feathr Feature Store on Home Credit\n",
    "\n",
    "This notebook illustrates the use of Feature Store to create a model for home credits. It includes these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite: Install Feathr\n",
    "\n",
    "Install Feathr using pip:\n",
    "\n",
    "`pip install -U feathr==0.7.1 pandavro scikit-learn`\n",
    "\n",
    "Or if you want to use the latest Feathr code from GitHub:\n",
    "\n",
    "`pip install -I git+https://github.com/linkedin/feathr.git#subdirectory=feathr_project pandavro scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U feathr==0.7.1 pandavro scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite: Configure the required environment\n",
    "\n",
    "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. If you use Feathr CLI to create a workspace, you should have a folder with a file called `feathr_config.yaml` in it with all the required configurations. Otherwise, update the configuration below.\n",
    "\n",
    "The code below will write this configuration string to a temporary location and load it to Feathr. Please still refer to [feathr_config.yaml](https://github.com/linkedin/feathr/blob/v0.7.2/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_PREFIX = '<RESOURCE_PREFIX>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "yaml_config = f\"\"\"\n",
    "# Please refer to https://github.com/linkedin/feathr/blob/v0.7.2/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml for explanations on the meaning of each field.\n",
    "api_version: 1\n",
    "project_config:\n",
    "  project_name: 'feathr_home_credit'\n",
    "  required_environment_variables:\n",
    "    - 'REDIS_PASSWORD'\n",
    "    - 'AZURE_CLIENT_ID'\n",
    "    - 'AZURE_TENANT_ID'\n",
    "    - 'AZURE_CLIENT_SECRET'\n",
    "offline_store:\n",
    "  adls:\n",
    "    adls_enabled: true\n",
    "  wasb:\n",
    "    wasb_enabled: true\n",
    "  s3:\n",
    "    s3_enabled: false\n",
    "    s3_endpoint: 's3.amazonaws.com'\n",
    "  jdbc:\n",
    "    jdbc_enabled: false\n",
    "    jdbc_database: 'feathrtestdb'\n",
    "    jdbc_table: 'feathrtesttable'\n",
    "spark_config:\n",
    "  spark_cluster: 'azure_synapse'\n",
    "  spark_result_output_parts: '1'\n",
    "  azure_synapse:\n",
    "    dev_url: \"https://{RESOURCE_PREFIX}spark.dev.azuresynapse.net\"\n",
    "    pool_name: \"spark31\"\n",
    "    # workspace dir for storing all the required configuration files and the jar resources\n",
    "    workspace_dir: \"abfss://{RESOURCE_PREFIX}fs@{RESOURCE_PREFIX}sto.dfs.core.windows.net/\"\n",
    "    executor_size: \"Small\"\n",
    "    executor_num: 4\n",
    "    feathr_runtime_location: wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar\n",
    "  databricks:\n",
    "    workspace_instance_url: 'https://adb-6885802458123232.12.azuredatabricks.net/'\n",
    "    workspace_token_value: ''\n",
    "    config_template: {{'run_name':'','new_cluster':{{'spark_version':'9.1.x-scala2.12','node_type_id':'Standard_D3_v2','num_workers':2,'spark_conf':{{}}}},'libraries':[{{'jar':''}}],'spark_jar_task':{{'main_class_name':'','parameters':['']}}}}\n",
    "    work_dir: 'dbfs:/feathr_getting_started'\n",
    "    feathr_runtime_location: wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar\n",
    "online_store:\n",
    "  redis:\n",
    "    host: '{RESOURCE_PREFIX}redis.redis.cache.windows.net'\n",
    "    port: 6380\n",
    "    ssl_enabled: True\n",
    "feature_registry:\n",
    "  purview:\n",
    "    type_system_initialization: true\n",
    "    purview_name: '{RESOURCE_PREFIX}purview'\n",
    "    delimiter: '__'\n",
    "\"\"\"\n",
    "tmp = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
    "with open(tmp.name, \"w\") as text_file:\n",
    "    text_file.write(yaml_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the data\n",
    "\n",
    "In this tutorial, we use Feathr Feature Store to create a model that predicts NYC Taxi fares. The dataset comes from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). The data is as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import pandavro as pdx\n",
    "from feathr import FeathrClient\n",
    "from feathr import BOOLEAN, FLOAT, INT32, ValueType, STRING\n",
    "from feathr import Feature, DerivedFeature, FeatureAnchor\n",
    "from feathr import BackfillTime, MaterializationSettings\n",
    "from feathr import FeatureQuery, ObservationSettings\n",
    "from feathr import RedisSink\n",
    "from feathr import INPUT_CONTEXT, HdfsSource\n",
    "from feathr import WindowAggTransformation\n",
    "from feathr import TypedKey\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup necessary environment variables\n",
    "\n",
    "You have to setup the environment variables in order to run this sample. More environment variables can be set by referring to [feathr_config.yaml](https://github.com/linkedin/feathr/blob/v0.7.2/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['REDIS_PASSWORD'] = ''\n",
    "os.environ['AZURE_CLIENT_ID'] = ''\n",
    "os.environ['AZURE_TENANT_ID'] = '' \n",
    "os.environ['AZURE_CLIENT_SECRET'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will initialize a feathr client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = FeathrClient(config_path=tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc pre-processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def add_tran_date_column(df: DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(\"2021-01-01 11:34:44\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_dummy_column(df: DataFrame) -> DataFrame:   \n",
    "    df = df.withColumn(\"DUMMY\", lit(\"dummy\"))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau balance pre-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_balance_preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    from pyspark import sql\n",
    "    \n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X')))\n",
    "    \n",
    "    # convert spark data frame to panda\n",
    "    df_org =  df.toPandas()\n",
    "    df_bureauBalanceRollingCreditLoan = df_org.copy()\n",
    "    \n",
    "    df_bureauBalanceRollingCreditLoan['STATUS'] = df_bureauBalanceRollingCreditLoan['STATUS'].replace(['X','C'],'0')\n",
    "    df_bureauBalanceRollingCreditLoan['STATUS'] = pd.to_numeric(df_bureauBalanceRollingCreditLoan['STATUS'])\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.groupby(\"SK_ID_BUREAU\")['STATUS'].agg(\n",
    "        lambda x: x.ewm(span=x.shape[0], adjust=False).mean().mean()\n",
    "    )\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.reset_index(name=\"CREDIT_STATUS_EMA_AVG\")\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.set_index('SK_ID_BUREAU')\n",
    "    df_result = pd.merge(df_org, df_bureauBalanceRollingCreditLoan, on=\"SK_ID_BUREAU\", how=\"left\")\n",
    "    \n",
    "    # convert panda to spark dataframe\n",
    "    spark_session = sql.SparkSession.builder.appName(\"pdf to sdf\").getOrCreate()\n",
    "        \n",
    "    return spark_session.createDataFrame(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau pre-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "def bureau_preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    from pyspark import sql\n",
    "    from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "    def bureauBalanceRollingCreditLoan(df):\n",
    "        df_final = df.copy()\n",
    "        df_final['STATUS'] = df_final['STATUS'].replace(['X','C'],'0')\n",
    "        df_final['STATUS'] = pd.to_numeric(df_final['STATUS'])\n",
    "        df_final = df_final.groupby(\"SK_ID_BUREAU\")['STATUS'].agg(\n",
    "            lambda x: x.ewm(span=x.shape[0], adjust=False).mean().mean()\n",
    "        )\n",
    "        df_final = df_final.reset_index(name=\"CREDIT_STATUS_EMA_AVG\")\n",
    "        df_final = df_final.set_index('SK_ID_BUREAU')\n",
    "        return df_final\n",
    "\n",
    "\n",
    "    def aggCountBureau(df):\n",
    "        agg = df.groupby(\"SK_ID_CURR\")\n",
    "        # count number of loans\n",
    "        df_final = pd.DataFrame(agg['SK_ID_CURR'].agg('count').reset_index(name='NUM_CREDIT_COUNT'))\n",
    "        # count number of loans prolonged\n",
    "        loans_prolonged = agg['CNT_CREDIT_PROLONG'].sum().reset_index(name='CREDIT_PROLONG_COUNT').set_index(\"SK_ID_CURR\")\n",
    "        df_final = df_final.join(loans_prolonged,on='SK_ID_CURR')\n",
    "        # count percentage of active loans\n",
    "        active_loans = agg['CREDIT_ACTIVE'].value_counts().reset_index(name='ACTIVE_LOANS_COUNT')\n",
    "        active_loans = active_loans[active_loans['CREDIT_ACTIVE'] == 'Active'][['SK_ID_CURR','ACTIVE_LOANS_COUNT']].set_index(\"SK_ID_CURR\")\n",
    "        df_final = df_final.join(active_loans,on='SK_ID_CURR')\n",
    "        df_final['ACTIVE_LOANS_PERCENT'] = df_final['ACTIVE_LOANS_COUNT']/df_final['NUM_CREDIT_COUNT']\n",
    "        df_final.drop([\"ACTIVE_LOANS_COUNT\"], axis=1, inplace=True)\n",
    "        df_final['ACTIVE_LOANS_PERCENT'] = df_final['ACTIVE_LOANS_PERCENT'].fillna(0)\n",
    "        # count credit type\n",
    "        # one hot encode\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe_fit = ohe.fit_transform(df[[\"CREDIT_TYPE\"]])\n",
    "        credit_type = pd.DataFrame(ohe_fit, columns = ohe.get_feature_names([\"CREDIT_TYPE\"]))\n",
    "        credit_type.insert(loc=0, column='SK_ID_CURR', value=df['SK_ID_CURR'].values)\n",
    "        credit_type = credit_type.groupby(\"SK_ID_CURR\").sum()\n",
    "        df_final = df_final.join(credit_type, on=\"SK_ID_CURR\")\n",
    "        df_final = df_final.set_index(\"SK_ID_CURR\")\n",
    "\n",
    "        return df_final\n",
    "    \n",
    "    # Average number of days between loans\n",
    "    # Average number of overdue days of overdue loans\n",
    "    def aggAvgBureau(df):\n",
    "        # convert this column to numeric\n",
    "        df['DAYS_CREDIT'] = pd.to_numeric(df['DAYS_CREDIT'])\n",
    "        agg = df.groupby('SK_ID_CURR')\n",
    "       \n",
    "        # average of CREDIT_DAY_OVERDUE\n",
    "        final_df = agg['CREDIT_DAY_OVERDUE'].mean().reset_index(name = \"CREDIT_DAY_OVERDUE_MEAN\")\n",
    "        # average of days between credits of DAYS_CREDIT\n",
    "        days_credit_between = pd.DataFrame(df['SK_ID_CURR'])\n",
    "        \n",
    "        days_credit_between['diff'] = agg['DAYS_CREDIT'].diff().values\n",
    "        days_credit_between = days_credit_between.groupby(\"SK_ID_CURR\")['diff'].mean().reset_index(name = 'DAYS_CREDIT_BETWEEN_MEAN')\n",
    "        days_credit_between.set_index(\"SK_ID_CURR\",inplace=True)\n",
    "        final_df = final_df.join(days_credit_between, on='SK_ID_CURR')\n",
    "        final_df = final_df.set_index(\"SK_ID_CURR\")\n",
    "        return final_df\n",
    "\n",
    "    #  ratio of AMT_CREDIT_SUM_DEBT to AMT_CREDIT_SUM created\n",
    "    def debtCreditRatio(df):\n",
    "        df['AMT_CREDIT_SUM_DEBT'] = pd.to_numeric(df['AMT_CREDIT_SUM_DEBT'])\n",
    "        df['AMT_CREDIT_SUM'] = pd.to_numeric(df['AMT_CREDIT_SUM'])\n",
    "        #get debt:credit ratio\n",
    "        df['DEBT_CREDIT_RATIO'] = df['AMT_CREDIT_SUM_DEBT']/df['AMT_CREDIT_SUM']\n",
    "        df_final = df.groupby('SK_ID_CURR')['DEBT_CREDIT_RATIO'].mean().reset_index(name='DEBT_CREDIT_RATIO')\n",
    "        df_final = df_final.set_index(\"SK_ID_CURR\")\n",
    "\n",
    "        df_final = df_final[df_final.columns.intersection(['SK_ID_CURR', 'DEBT_CREDIT_RATIO'])]\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    # add a TRAN_DATE column with a static date\n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X')))\n",
    "    df_org = df.toPandas()\n",
    "        \n",
    "    df_aggCountBureau = aggCountBureau(df_org)\n",
    "    df_aggAvgInstalments = aggAvgBureau(df_org)\n",
    "    df_debtCreditRatio = debtCreditRatio(df_org)\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    dfs.append(df_aggCountBureau)\n",
    "    dfs.append(df_aggAvgInstalments)\n",
    "    dfs.append(df_debtCreditRatio)\n",
    "\n",
    "    df_result = dfs.pop()\n",
    "    while dfs:\n",
    "        df_result = df_result.join(dfs.pop(),on='SK_ID_CURR')\n",
    "    \n",
    "    # results df would be merge to the original df\n",
    "    df_result = pd.merge(df_org, df_result, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    # merging df with same column name would result a columnname with a suffix of `_x` and `_y`.\n",
    "    # Renaming the column name with suffix `_x` to retain the original column name\n",
    "    df_result.columns = df_result.columns.str.rstrip(\"_x\")\n",
    "\n",
    "    # convert panda to spark dataframe\n",
    "    spark_session = sql.SparkSession.builder.appName(\"pdf to sdf\").getOrCreate()\n",
    "    \n",
    "    return spark_session.createDataFrame(df_result)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Features with Feathr:\n",
    "\n",
    "### Bureau Dataset\n",
    "1. parent dataset: bureau.csv \n",
    "    1. count aggregation features created\n",
    "    1. average aggregation features created\n",
    "    1. debt:credit ratio feature created\n",
    "1. child dataset: bureau_balance.csv\n",
    "    1. rolling window credit loan status feature will be created and joined to parent dataset\n",
    "1. combinig/joining both datasets, which will be aggregated in line with primary key (\"SK_ID_CURR) of application_train (target dataframe) with the following features:    \n",
    "    1. count aggregation features created\n",
    "    1. average aggregation features created\n",
    "    1. debt:credit ratio feature created\n",
    "    1. rolling window credit loan status feature will be created and joined to parent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two datasource pointing to same csv, limitation that you could not mix\n",
    "# pass through and aggregated features. By separating, it must have different datasource (datasource name)\n",
    "\n",
    "# source for pass through features\n",
    "# \"TRAN_DATE\" column created on on the \"datasource_prepocessing\" method.\n",
    "bureau_source_core = HdfsSource(name=\"bureauSourceCore\",\n",
    "                          path=f\"abfss://{RESOURCE_PREFIX}fs@{RESOURCE_PREFIX}sto.dfs.core.windows.net/home_credit_data/bureau.csv\",\n",
    "                          preprocessing=bureau_preprocessing,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "# key definition for bureau datasource\n",
    "key_SK_ID_BUREAU = TypedKey(key_column=\"SK_ID_BUREAU\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"SK ID Bureau\",\n",
    "                       full_name=\"bureau.SK_ID_BUREAU\")\n",
    "\n",
    "key_SK_ID_CURR = TypedKey(key_column=\"SK_ID_CURR\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"SK ID CURR\",\n",
    "                       full_name=\"bureau.SK_ID_CURR\")\n",
    "\n",
    "# pass through columns of BUREAU datasource CSV\n",
    "f_SK_ID_CURR = Feature(name=\"f_SK_ID_CURR\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=INT32, \n",
    "                        transform=\"SK_ID_CURR\")\n",
    "\n",
    "f_SK_ID_BUREAU  = Feature(name=\"f_SK_ID_BUREAU\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"SK_ID_BUREAU\")\n",
    "\n",
    "f_CREDIT_ACTIVE = Feature(name=\"f_CREDIT_ACTIVE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_ACTIVE\")\n",
    "\n",
    "f_CREDIT_CURRENCY = Feature(name=\"f_CREDIT_CURRENCY\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_CURRENCY\")\n",
    "\n",
    "f_DAYS_CREDIT = Feature(name=\"f_DAYS_CREDIT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT\")\n",
    "\n",
    "f_CREDIT_DAY_OVERDUE = Feature(name=\"f_CREDIT_DAY_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_DAY_OVERDUE\")\n",
    "\n",
    "f_DAYS_CREDIT_ENDDATE = Feature(name=\"f_DAYS_CREDIT_ENDDATE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT_ENDDATE\")\n",
    "\n",
    "f_DAYS_ENDDATE_FACT = Feature(name=\"f_DAYS_ENDDATE_FACT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_ENDDATE_FACT\")\n",
    "\n",
    "f_AMT_CREDIT_MAX_OVERDUE = Feature(name=\"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_MAX_OVERDUE\")\n",
    "\n",
    "f_CNT_CREDIT_PROLONG = Feature(name=\"f_CNT_CREDIT_PROLONG\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CNT_CREDIT_PROLONG\")\n",
    "\n",
    "f_AMT_CREDIT_SUM = Feature(name=\"f_AMT_CREDIT_SUM\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_DEBT = Feature(name=\"f_AMT_CREDIT_SUM_DEBT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_DEBT\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_LIMIT = Feature(name=\"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_LIMIT\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_OVERDUE = Feature(name=\"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_OVERDUE\")\n",
    "\n",
    "f_CREDIT_TYPE = Feature(name=\"f_CREDIT_TYPE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_TYPE\")\n",
    "\n",
    "f_DAYS_CREDIT_UPDATE = Feature(name=\"f_DAYS_CREDIT_UPDATE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT_UPDATE\")\n",
    "\n",
    "f_AMT_ANNUITY = Feature(name=\"f_AMT_ANNUITY\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_ANNUITY\")\n",
    "\n",
    "\n",
    "\n",
    "f_NUM_CREDIT_COUNT = Feature(name=\"f_NUM_CREDIT_COUNT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"NUM_CREDIT_COUNT\")\n",
    "\n",
    "f_DEBT_CREDIT_RATIO = Feature(name=\"f_DEBT_CREDIT_RATIO\",\n",
    "                        key=key_SK_ID_BUREAU,\n",
    "                        feature_type=STRING,\n",
    "                        transform=\"DEBT_CREDIT_RATIO\")\n",
    "\n",
    "\n",
    "features_bureau_source_core=[\n",
    "  f_SK_ID_CURR,\n",
    "  f_SK_ID_BUREAU,\n",
    "  f_CREDIT_ACTIVE,\n",
    "  f_CREDIT_CURRENCY,\n",
    "  f_DAYS_CREDIT,\n",
    "  f_CREDIT_DAY_OVERDUE,\n",
    "  f_DAYS_CREDIT_ENDDATE,\n",
    "  f_DAYS_ENDDATE_FACT,\n",
    "  f_AMT_CREDIT_MAX_OVERDUE,\n",
    "  f_CNT_CREDIT_PROLONG,\n",
    "  f_AMT_CREDIT_SUM,\n",
    "  f_AMT_CREDIT_SUM_DEBT,\n",
    "  f_AMT_CREDIT_SUM_LIMIT,\n",
    "  f_AMT_CREDIT_SUM_OVERDUE,\n",
    "  f_CREDIT_TYPE,\n",
    "  f_DAYS_CREDIT_UPDATE,\n",
    "  f_AMT_ANNUITY,\n",
    "\n",
    "  f_NUM_CREDIT_COUNT,\n",
    "  f_DEBT_CREDIT_RATIO,\n",
    "  ]\n",
    "\n",
    "anchor_bureau_source_core = FeatureAnchor(name=\"anchor_bureau_source_core\",\n",
    "                                source=bureau_source_core, #INPUT_CONTEXT,\n",
    "                                features=features_bureau_source_core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for aggregated features of BUREAU\n",
    "bureau_source_agg = HdfsSource(name=\"bureauSourceAgg\",\n",
    "                          path=f\"abfss://{RESOURCE_PREFIX}fs@{RESOURCE_PREFIX}sto.dfs.core.windows.net/home_credit_data/bureau.csv\",\n",
    "                          preprocessing=add_tran_date_column,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for aggregated features\n",
    "bureau_balance_source_core = HdfsSource(name=\"bureauBalanceSourceCore\",\n",
    "                          path=f\"abfss://{RESOURCE_PREFIX}fs@{RESOURCE_PREFIX}sto.dfs.core.windows.net/home_credit_data/bureau_balance.csv\",\n",
    "                          preprocessing=bureau_balance_preprocessing,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "f_MONTHS_BALANCE  = Feature(name=\"f_MONTHS_BALANCE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"MONTHS_BALANCE\")\n",
    "\n",
    "f_STATUS  = Feature(name=\"f_STATUS\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING,\n",
    "                        transform=\"STATUS\")\n",
    "\n",
    "f_CREDIT_STATUS_EMA_AVG  = Feature(name=\"f_CREDIT_STATUS_EMA_AVG\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING,\n",
    "                        transform=\"CREDIT_STATUS_EMA_AVG\")\n",
    "                        \n",
    "features_bureau_balance_source_core=[\n",
    "  f_MONTHS_BALANCE,\n",
    "  f_STATUS,\n",
    "  f_CREDIT_STATUS_EMA_AVG\n",
    "  ]\n",
    "\n",
    "anchor_bureau_balance_source_core = FeatureAnchor(name=\"anchor_bureau_balance_source_core\",\n",
    "                                source=bureau_balance_source_core,\n",
    "                                features=features_bureau_balance_source_core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we need to build those features so that it can be consumed later. Note that we have to build both the \"anchor\" and the \"derived\" features (which is not anchored to a source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[\n",
    "        anchor_bureau_source_core,\n",
    "        anchor_bureau_balance_source_core,\n",
    "        ], \n",
    "    derived_feature_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data using point-in-time correct feature join\n",
    "\n",
    "A training dataset usually contains entity id columns, multiple feature columns, event timestamp column and label/target column. \n",
    "\n",
    "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n",
    "what features and how these features should be joined to the observation data. The feature join config file mainly contains: \n",
    "\n",
    "1. The path of a dataset as the 'spine' for the to-be-created training dataset. We call this input 'spine' dataset the 'observation'\n",
    "   dataset. Typically, each row of the observation data contains: \n",
    "   a) Column(s) representing entity id(s), which will be used as the join key to look up(join) feature value. \n",
    "   b) A column representing the event time of the row. By default, Feathr will make sure the feature values joined have\n",
    "   a timestamp earlier than it, ensuring no data leakage in the resulting training dataset. \n",
    "   c) Other columns will be simply pass through onto the output training dataset.\n",
    "2. The key fields from the observation data, which are used to joined with the feature data.\n",
    "3. List of feature names to be joined with the observation data. The features must be defined in the feature\n",
    "   definition configs.\n",
    "4. The time information of the observation data used to compare with the feature's timestamp during the join.\n",
    "\n",
    "Create training dataset via:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_queries = [\n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            \"f_SK_ID_CURR\",\n",
    "            \"f_SK_ID_BUREAU\",\n",
    "            \"f_CREDIT_ACTIVE\",\n",
    "            \"f_CREDIT_CURRENCY\",\n",
    "            \"f_DAYS_CREDIT\",\n",
    "            \"f_CREDIT_DAY_OVERDUE\",\n",
    "            \"f_DAYS_CREDIT_ENDDATE\",\n",
    "            \"f_DAYS_ENDDATE_FACT\",\n",
    "            \"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "            \"f_CNT_CREDIT_PROLONG\",\n",
    "            \"f_AMT_CREDIT_SUM\",\n",
    "            \"f_AMT_CREDIT_SUM_DEBT\",\n",
    "            \"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "            \"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "            \"f_CREDIT_TYPE\",\n",
    "            \"f_DAYS_CREDIT_UPDATE\",\n",
    "            \"f_AMT_ANNUITY\",\n",
    "\n",
    "            \"f_NUM_CREDIT_COUNT\",\n",
    "            \"f_DEBT_CREDIT_RATIO\",\n",
    "        ], key=key_SK_ID_BUREAU),\n",
    "    \n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            \"f_MONTHS_BALANCE\",\n",
    "            \"f_STATUS\",\n",
    "            \"f_CREDIT_STATUS_EMA_AVG\"\n",
    "        ], key=key_SK_ID_BUREAU)\n",
    "]\n",
    "\n",
    "# spine dataset was created manually, it's the same as the bureau.csv \n",
    "# with constant event_timetamp_column\n",
    "settings = ObservationSettings(\n",
    "    observation_path=f\"abfss://{RESOURCE_PREFIX}fs@{RESOURCE_PREFIX}sto.dfs.core.windows.net/home_credit_data/bureau.csv\",\n",
    "    event_timestamp_column=\"1609472084\",\n",
    "    timestamp_format=\"epoch\"\n",
    ")\n",
    "\n",
    "# output would be in output_bureau.avro\n",
    "client.get_offline_features(observation_settings=settings,\n",
    "                            feature_query=feature_queries,\n",
    "                            output_path=f\"abfss://{RESOURCE_PREFIX}fs@{RESOURCE_PREFIX}sto.dfs.core.windows.net/home_credit_data/output_bureau.avro\")\n",
    "client.wait_job_to_finish(timeout_sec=7200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the result and show the result\n",
    "\n",
    "Let's use the helper function `get_result_df` to download the result and view it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "def get_result_df(client: FeathrClient) -> pd.DataFrame:\n",
    "    \"\"\"Download the job result dataset from cloud as a Pandas dataframe.\"\"\"\n",
    "    res_url = client.get_job_result_uri(block=True, timeout_sec=600)\n",
    "    tmp_dir = \"../output_bureau.avro\"\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    client.feathr_spark_launcher.download_result(result_path=res_url, local_folder=tmp_dir)\n",
    "    dataframe_list = []\n",
    "    # assuming the result are in avro format\n",
    "    for file in glob.glob(os.path.join(tmp_dir, '*.avro')):\n",
    "        dataframe_list.append(pdx.read_avro(file))\n",
    "    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n",
    "    return vertical_concat_df\n",
    "\n",
    "df_res = get_result_df(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pd.option_context('display.max_columns', 50, 'display.max_rows', 1000):\n",
    "   print(df_res.columns.values.tolist())\n",
    "   print(df_res[[\n",
    "      \"f_SK_ID_CURR\",\n",
    "      \"f_SK_ID_BUREAU\",\n",
    "      \"f_CREDIT_ACTIVE\",\n",
    "      \"f_CREDIT_CURRENCY\",\n",
    "      \"f_DAYS_CREDIT\",\n",
    "      \"f_CREDIT_DAY_OVERDUE\",\n",
    "      \"f_DAYS_CREDIT_ENDDATE\",\n",
    "      \"f_DAYS_ENDDATE_FACT\",\n",
    "      \"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "      \"f_CNT_CREDIT_PROLONG\",\n",
    "      \"f_AMT_CREDIT_SUM\",\n",
    "      \"f_AMT_CREDIT_SUM_DEBT\",\n",
    "      \"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "      \"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "      \"f_CREDIT_TYPE\",\n",
    "      \"f_DAYS_CREDIT_UPDATE\",\n",
    "      \"f_AMT_ANNUITY\",\n",
    "\n",
    "      \"f_SK_ID_BUREAU\",\n",
    "      \"f_MONTHS_BALANCE\",\n",
    "      \"f_STATUS\",\n",
    "      \"f_CREDIT_STATUS_EMA_AVG\",\n",
    "\n",
    "      \"f_NUM_CREDIT_COUNT\",\n",
    "      \"f_DEBT_CREDIT_RATIO\"\n",
    "   ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backfill_time = BackfillTime(start=datetime(2020, 5, 20), \n",
    "                             end=datetime(2020, 5, 20), \n",
    "                             step=timedelta(days=1))\n",
    "redisSink = RedisSink(table_name=\"homeCreditDemoFeature\")\n",
    "settings = MaterializationSettings(name=\"homeCreditFeatureSetting\",\n",
    "                                   backfill_time=backfill_time,\n",
    "                                   sinks=[redisSink],\n",
    "                                   feature_names=[\"f_NUM_CREDIT_COUNT\"])\n",
    "\n",
    "client.materialize_features(settings)\n",
    "client.wait_job_to_finish(timeout_sec=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_online_features('homeCreditDemoFeature', \n",
    "                           '6841943', \n",
    "                           ['f_NUM_CREDIT_COUNT'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd8e05a3b29cb52c25a673b02199ba49a1ae4abbf3dc61fdb468ec9ed0117842"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
