{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feathr Feature Store on Home Credit\n",
    "\n",
    "This notebook illustrates the use of Feature Store to create a model for home credits. It includes these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite: Install Feathr\n",
    "\n",
    "Install Feathr using pip:\n",
    "\n",
    "`pip install -U feathr pandavro scikit-learn`\n",
    "\n",
    "Or if you want to use the latest Feathr code from GitHub:\n",
    "\n",
    "`pip install -I git+https://github.com/linkedin/feathr.git#subdirectory=feathr_project pandavro scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feathr in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (0.5.1)\n",
      "Requirement already satisfied: pandavro in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (1.7.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: Click in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (8.1.3)\n",
      "Requirement already satisfied: python-snappy in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.6.1)\n",
      "Requirement already satisfied: databricks-cli in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.16.6)\n",
      "Requirement already satisfied: pyyaml in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (6.0)\n",
      "Requirement already satisfied: loguru in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.6.0)\n",
      "Requirement already satisfied: pyhocon in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.3.59)\n",
      "Requirement already satisfied: google-api-python-client>=2.41.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (2.48.0)\n",
      "Requirement already satisfied: py4j in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.10.9.3)\n",
      "Requirement already satisfied: redis in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (4.3.1)\n",
      "Requirement already satisfied: pyspark>=3.1.2 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (3.2.1)\n",
      "Requirement already satisfied: avro in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (1.11.0)\n",
      "Requirement already satisfied: graphlib-backport in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (1.0.3)\n",
      "Requirement already satisfied: google>=3.0.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (4.64.0)\n",
      "Requirement already satisfied: requests in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (2.27.1)\n",
      "Requirement already satisfied: azure-keyvault-secrets in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (4.4.0)\n",
      "Requirement already satisfied: azure-synapse-spark in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.7.0)\n",
      "Requirement already satisfied: confluent-kafka in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (1.8.2)\n",
      "Requirement already satisfied: pyarrow in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (8.0.0)\n",
      "Requirement already satisfied: Jinja2 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (3.1.2)\n",
      "Requirement already satisfied: azure-core<=1.22.1 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (1.22.1)\n",
      "Requirement already satisfied: deltalake in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.5.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (4.2.0)\n",
      "Requirement already satisfied: azure-identity in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (1.10.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake>=12.5.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (12.6.0)\n",
      "Requirement already satisfied: pyapacheatlas in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (0.12.0)\n",
      "Requirement already satisfied: pandas in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from feathr) (1.2.5)\n",
      "Requirement already satisfied: fastavro==1.5.1 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from pandavro) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from pandavro) (1.22.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from azure-core<=1.22.1->feathr) (1.16.0)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.10.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from azure-storage-file-datalake>=12.5.0->feathr) (12.11.0)\n",
      "Requirement already satisfied: msrest>=0.6.21 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from azure-storage-file-datalake>=12.5.0->feathr) (0.6.21)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google>=3.0.0->feathr) (4.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (4.1.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (0.20.4)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (2.7.3)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.16.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (2.6.6)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (0.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from pandas->feathr) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from pandas->feathr) (2.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from requests->feathr) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from requests->feathr) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from requests->feathr) (2022.5.18)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from requests->feathr) (2.0.12)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from azure-identity->feathr) (37.0.2)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from azure-identity->feathr) (1.0.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from azure-identity->feathr) (1.17.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from azure-keyvault-secrets->feathr) (1.1.28)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from databricks-cli->feathr) (2.4.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from databricks-cli->feathr) (0.8.9)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from databricks-cli->feathr) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from Jinja2->feathr) (2.1.1)\n",
      "Requirement already satisfied: openpyxl>=3.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from pyapacheatlas->feathr) (3.0.9)\n",
      "Requirement already satisfied: pyparsing~=2.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from pyhocon->feathr) (2.4.7)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from redis->feathr) (4.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.3 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from redis->feathr) (1.2.13)\n",
      "Requirement already satisfied: packaging>=20.4 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from redis->feathr) (21.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from cryptography>=2.5->azure-identity->feathr) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from deprecated>=1.2.3->redis->feathr) (1.14.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=2.41.0->feathr) (1.56.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=2.41.0->feathr) (3.20.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=2.41.0->feathr) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=2.41.0->feathr) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=2.41.0->feathr) (5.1.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->feathr) (2.4.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from msrest>=0.6.21->azure-storage-file-datalake>=12.5.0->feathr) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from msrest>=0.6.21->azure-storage-file-datalake>=12.5.0->feathr) (1.3.1)\n",
      "Requirement already satisfied: et-xmlfile in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from openpyxl>=3.0->pyapacheatlas->feathr) (1.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from beautifulsoup4->google>=3.0.0->feathr) (2.3.2.post1)\n",
      "Requirement already satisfied: pycparser in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity->feathr) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/dasorbit/projects/merlion-feature-store-scenario/.venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.16.0->google-api-python-client>=2.41.0->feathr) (0.4.8)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U feathr pandavro scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite: Configure the required environment\n",
    "\n",
    "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. If you use Feathr CLI to create a workspace, you should have a folder with a file called `feathr_config.yaml` in it with all the required configurations. Otherwise, update the configuration below.\n",
    "\n",
    "The code below will write this configuration string to a temporary location and load it to Feathr. Please still refer to [feathr_config.yaml](https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "yaml_config = \"\"\"\n",
    "# Please refer to https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml for explanations on the meaning of each field.\n",
    "api_version: 1\n",
    "project_config:\n",
    "  project_name: 'feathr_home_credit'\n",
    "  required_environment_variables:\n",
    "    - 'REDIS_PASSWORD'\n",
    "    - 'AZURE_CLIENT_ID'\n",
    "    - 'AZURE_TENANT_ID'\n",
    "    - 'AZURE_CLIENT_SECRET'\n",
    "offline_store:\n",
    "  adls:\n",
    "    adls_enabled: tru\n",
    "  wasb:\n",
    "    wasb_enabled: true\n",
    "  s3:\n",
    "    s3_enabled: false\n",
    "    s3_endpoint: 's3.amazonaws.com'\n",
    "  jdbc:\n",
    "    jdbc_enabled: false\n",
    "    jdbc_database: 'feathrtestdb'\n",
    "    jdbc_table: 'feathrtesttable'\n",
    "  snowflake:\n",
    "    url: \"dqllago-ol19457.snowflakecomputing.com\"\n",
    "    user: \"feathrintegration\"\n",
    "    role: \"ACCOUNTADMIN\"\n",
    "spark_config:\n",
    "  spark_cluster: 'azure_synapse'\n",
    "  spark_result_output_parts: '1'\n",
    "  azure_synapse:\n",
    "    dev_url: \"https://feathrhomecreditcaspark.dev.azuresynapse.net\"\n",
    "    pool_name: \"spark31\"\n",
    "    # workspace dir for storing all the required configuration files and the jar resources\n",
    "    workspace_dir: \"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/\"\n",
    "    executor_size: \"Small\"\n",
    "    executor_num: 4\n",
    "    feathr_runtime_location: wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar\n",
    "  databricks:\n",
    "    workspace_instance_url: 'https://adb-6885802458123232.12.azuredatabricks.net/'\n",
    "    workspace_token_value: ''\n",
    "    config_template: {'run_name':'','new_cluster':{'spark_version':'9.1.x-scala2.12','node_type_id':'Standard_D3_v2','num_workers':2,'spark_conf':{}},'libraries':[{'jar':''}],'spark_jar_task':{'main_class_name':'','parameters':['']}}\n",
    "    work_dir: 'dbfs:/feathr_getting_started'\n",
    "    feathr_runtime_location: wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar\n",
    "online_store:\n",
    "  redis:\n",
    "    host: 'feathrhomecreditcaredis.redis.cache.windows.net'\n",
    "    port: 6380\n",
    "    ssl_enabled: True\n",
    "feature_registry:\n",
    "  purview:\n",
    "    type_system_initialization: true\n",
    "    purview_name: 'feathrhomecreditcapurview'\n",
    "    delimiter: '__'\n",
    "\"\"\"\n",
    "tmp = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
    "with open(tmp.name, \"w\") as text_file:\n",
    "    text_file.write(yaml_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the data\n",
    "\n",
    "In this tutorial, we use Feathr Feature Store to create a model that predicts NYC Taxi fares. The dataset comes from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). The data is as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import pandavro as pdx\n",
    "from feathr import FeathrClient\n",
    "from feathr import BOOLEAN, FLOAT, INT32, ValueType, STRING\n",
    "from feathr import Feature, DerivedFeature, FeatureAnchor\n",
    "from feathr import BackfillTime, MaterializationSettings\n",
    "from feathr import FeatureQuery, ObservationSettings\n",
    "from feathr import RedisSink\n",
    "from feathr import INPUT_CONTEXT, HdfsSource\n",
    "from feathr import WindowAggTransformation\n",
    "from feathr import TypedKey\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup necessary environment variables\n",
    "\n",
    "You have to setup the environment variables in order to run this sample. More environment variables can be set by referring to [feathr_config.yaml](https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['REDIS_PASSWORD'] = ''\n",
    "os.environ['AZURE_CLIENT_ID'] = ''\n",
    "os.environ['AZURE_TENANT_ID'] = '' \n",
    "os.environ['AZURE_CLIENT_SECRET'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will initialize a feathr client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = FeathrClient(config_path=tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc pre-processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "# Add pre-processing functions\n",
    "# 1. On the fly datetime field\n",
    "#   1. Create a pre-processing function to add a timefield on the\n",
    "#   2. Attach the preprocessing method in the data source\n",
    "\n",
    "\n",
    "def add_tran_date_column(df: DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(\"2021-01-01 11:34:44\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_dummy_column(df: DataFrame) -> DataFrame:   \n",
    "    df = df.withColumn(\"DUMMY\", lit(\"dummy\"))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau balance pre-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_balance_preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    from pyspark import sql\n",
    "    \n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X')))\n",
    "    \n",
    "    # convert spark data frame to panda\n",
    "    df_org =  df.toPandas()\n",
    "    df_bureauBalanceRollingCreditLoan = df_org.copy()\n",
    "    \n",
    "    df_bureauBalanceRollingCreditLoan['STATUS'] = df_bureauBalanceRollingCreditLoan['STATUS'].replace(['X','C'],'0')\n",
    "    df_bureauBalanceRollingCreditLoan['STATUS'] = pd.to_numeric(df_bureauBalanceRollingCreditLoan['STATUS'])\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.groupby(\"SK_ID_BUREAU\")['STATUS'].agg(\n",
    "        lambda x: x.ewm(span=x.shape[0], adjust=False).mean().mean()\n",
    "    )\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.reset_index(name=\"CREDIT_STATUS_EMA_AVG\")\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.set_index('SK_ID_BUREAU')\n",
    "    df_result = pd.merge(df_org, df_bureauBalanceRollingCreditLoan, on=\"SK_ID_BUREAU\", how=\"left\")\n",
    "    \n",
    "    # convert panda to spark dataframe\n",
    "    spark_session = sql.SparkSession.builder.appName(\"pdf to sdf\").getOrCreate()\n",
    "        \n",
    "    return spark_session.createDataFrame(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau pre-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "def bureau_preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    from pyspark import sql\n",
    "    from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "    def bureauBalanceRollingCreditLoan(df):\n",
    "        df_final = df.copy()\n",
    "        df_final['STATUS'] = df_final['STATUS'].replace(['X','C'],'0')\n",
    "        df_final['STATUS'] = pd.to_numeric(df_final['STATUS'])\n",
    "        df_final = df_final.groupby(\"SK_ID_BUREAU\")['STATUS'].agg(\n",
    "            lambda x: x.ewm(span=x.shape[0], adjust=False).mean().mean()\n",
    "        )\n",
    "        df_final = df_final.reset_index(name=\"CREDIT_STATUS_EMA_AVG\")\n",
    "        df_final = df_final.set_index('SK_ID_BUREAU')\n",
    "        return df_final\n",
    "\n",
    "\n",
    "    def aggCountBureau(df):\n",
    "        agg = df.groupby(\"SK_ID_CURR\")\n",
    "        # count number of loans\n",
    "        df_final = pd.DataFrame(agg['SK_ID_CURR'].agg('count').reset_index(name='NUM_CREDIT_COUNT'))\n",
    "        # count number of loans prolonged\n",
    "        loans_prolonged = agg['CNT_CREDIT_PROLONG'].sum().reset_index(name='CREDIT_PROLONG_COUNT').set_index(\"SK_ID_CURR\")\n",
    "        df_final = df_final.join(loans_prolonged,on='SK_ID_CURR')\n",
    "        # count percentage of active loans\n",
    "        active_loans = agg['CREDIT_ACTIVE'].value_counts().reset_index(name='ACTIVE_LOANS_COUNT')\n",
    "        active_loans = active_loans[active_loans['CREDIT_ACTIVE'] == 'Active'][['SK_ID_CURR','ACTIVE_LOANS_COUNT']].set_index(\"SK_ID_CURR\")\n",
    "        df_final = df_final.join(active_loans,on='SK_ID_CURR')\n",
    "        df_final['ACTIVE_LOANS_PERCENT'] = df_final['ACTIVE_LOANS_COUNT']/df_final['NUM_CREDIT_COUNT']\n",
    "        df_final.drop([\"ACTIVE_LOANS_COUNT\"], axis=1, inplace=True)\n",
    "        df_final['ACTIVE_LOANS_PERCENT'] = df_final['ACTIVE_LOANS_PERCENT'].fillna(0)\n",
    "        # count credit type\n",
    "        # one hot encode\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe_fit = ohe.fit_transform(df[[\"CREDIT_TYPE\"]])\n",
    "        credit_type = pd.DataFrame(ohe_fit, columns = ohe.get_feature_names([\"CREDIT_TYPE\"]))\n",
    "        credit_type.insert(loc=0, column='SK_ID_CURR', value=df['SK_ID_CURR'].values)\n",
    "        credit_type = credit_type.groupby(\"SK_ID_CURR\").sum()\n",
    "        df_final = df_final.join(credit_type, on=\"SK_ID_CURR\")\n",
    "        df_final = df_final.set_index(\"SK_ID_CURR\")\n",
    "\n",
    "        return df_final\n",
    "    \n",
    "    # Average number of days between loans\n",
    "    # Average number of overdue days of overdue loans\n",
    "    def aggAvgBureau(df):\n",
    "        # convert this column to numeric\n",
    "        df['DAYS_CREDIT'] = pd.to_numeric(df['DAYS_CREDIT'])\n",
    "        agg = df.groupby('SK_ID_CURR')\n",
    "       \n",
    "        # average of CREDIT_DAY_OVERDUE\n",
    "        final_df = agg['CREDIT_DAY_OVERDUE'].mean().reset_index(name = \"CREDIT_DAY_OVERDUE_MEAN\")\n",
    "        # average of days between credits of DAYS_CREDIT\n",
    "        days_credit_between = pd.DataFrame(df['SK_ID_CURR'])\n",
    "        \n",
    "        days_credit_between['diff'] = agg['DAYS_CREDIT'].diff().values\n",
    "        days_credit_between = days_credit_between.groupby(\"SK_ID_CURR\")['diff'].mean().reset_index(name = 'DAYS_CREDIT_BETWEEN_MEAN')\n",
    "        days_credit_between.set_index(\"SK_ID_CURR\",inplace=True)\n",
    "        final_df = final_df.join(days_credit_between, on='SK_ID_CURR')\n",
    "        final_df = final_df.set_index(\"SK_ID_CURR\")\n",
    "        return final_df\n",
    "\n",
    "    #  ratio of AMT_CREDIT_SUM_DEBT to AMT_CREDIT_SUM created\n",
    "    def debtCreditRatio(df):\n",
    "        df['AMT_CREDIT_SUM_DEBT'] = pd.to_numeric(df['AMT_CREDIT_SUM_DEBT'])\n",
    "        df['AMT_CREDIT_SUM'] = pd.to_numeric(df['AMT_CREDIT_SUM'])\n",
    "        #get debt:credit ratio\n",
    "        df['DEBT_CREDIT_RATIO'] = df['AMT_CREDIT_SUM_DEBT']/df['AMT_CREDIT_SUM']\n",
    "        df_final = df.groupby('SK_ID_CURR')['DEBT_CREDIT_RATIO'].mean().reset_index(name='DEBT_CREDIT_RATIO')\n",
    "        df_final = df_final.set_index(\"SK_ID_CURR\")\n",
    "\n",
    "        df_final = df_final[df_final.columns.intersection(['SK_ID_CURR', 'DEBT_CREDIT_RATIO'])]\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    # add a TRAN_DATE column with a static date\n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X')))\n",
    "    df_org = df.toPandas()\n",
    "        \n",
    "    df_aggCountBureau = aggCountBureau(df_org)\n",
    "    df_aggAvgInstalments = aggAvgBureau(df_org)\n",
    "    df_debtCreditRatio = debtCreditRatio(df_org)\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    dfs.append(df_aggCountBureau)\n",
    "    dfs.append(df_aggAvgInstalments)\n",
    "    dfs.append(df_debtCreditRatio)\n",
    "\n",
    "    df_result = dfs.pop()\n",
    "    while dfs:\n",
    "        df_result = df_result.join(dfs.pop(),on='SK_ID_CURR')\n",
    "    \n",
    "    # results df would be merge to the original df\n",
    "    df_result = pd.merge(df_org, df_result, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    # merging df with same column name would result a columnname with a suffix of `_x` and `_y`.\n",
    "    # Renaming the column name with suffix `_x` to retain the original column name\n",
    "    df_result.columns = df_result.columns.str.rstrip(\"_x\")\n",
    "\n",
    "    # convert panda to spark dataframe\n",
    "    spark_session = sql.SparkSession.builder.appName(\"pdf to sdf\").getOrCreate()\n",
    "    \n",
    "    return spark_session.createDataFrame(df_result)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Features with Feathr:\n",
    "\n",
    "### Bureau Dataset\n",
    "1. parent dataset: bureau.csv \n",
    "    1. count aggregation features created\n",
    "    1. average aggregation features created\n",
    "    1. debt:credit ratio feature created\n",
    "1. child dataset: bureau_balance.csv\n",
    "    1. rolling window credit loan status feature will be created and joined to parent dataset\n",
    "1. combinig/joining both datasets, which will be aggregated in line with primary key (\"SK_ID_CURR) of application_train (target dataframe) with the following features:    \n",
    "    1. count aggregation features created\n",
    "    1. average aggregation features created\n",
    "    1. debt:credit ratio feature created\n",
    "    1. rolling window credit loan status feature will be created and joined to parent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two datasource pointing to same csv, limitation that you could not mix\n",
    "# pass through and aggregated features. By separating, it must have different datasource (datasource name)\n",
    "\n",
    "# source for pass through features\n",
    "# \"TRAN_DATE\" column created on on the \"datasource_prepocessing\" method.\n",
    "bureau_source_core = HdfsSource(name=\"bureauSourceCore\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau.csv\",\n",
    "                          preprocessing=bureau_preprocessing,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "# key definition for bureau datasource\n",
    "key_SK_ID_BUREAU = TypedKey(key_column=\"SK_ID_BUREAU\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"SK ID Bureau\",\n",
    "                       full_name=\"bureau.SK_ID_BUREAU\")\n",
    "\n",
    "key_SK_ID_CURR = TypedKey(key_column=\"SK_ID_CURR\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"SK ID CURR\",\n",
    "                       full_name=\"bureau.SK_ID_CURR\")\n",
    "\n",
    "# pass through columns of BUREAU datasource CSV\n",
    "f_SK_ID_CURR = Feature(name=\"f_SK_ID_CURR\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=INT32, \n",
    "                        transform=\"SK_ID_CURR\")\n",
    "\n",
    "f_SK_ID_BUREAU  = Feature(name=\"f_SK_ID_BUREAU\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"SK_ID_BUREAU\")\n",
    "\n",
    "f_CREDIT_ACTIVE = Feature(name=\"f_CREDIT_ACTIVE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_ACTIVE\")\n",
    "\n",
    "f_CREDIT_CURRENCY = Feature(name=\"f_CREDIT_CURRENCY\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_CURRENCY\")\n",
    "\n",
    "f_DAYS_CREDIT = Feature(name=\"f_DAYS_CREDIT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT\")\n",
    "\n",
    "f_CREDIT_DAY_OVERDUE = Feature(name=\"f_CREDIT_DAY_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_DAY_OVERDUE\")\n",
    "\n",
    "f_DAYS_CREDIT_ENDDATE = Feature(name=\"f_DAYS_CREDIT_ENDDATE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT_ENDDATE\")\n",
    "\n",
    "f_DAYS_ENDDATE_FACT = Feature(name=\"f_DAYS_ENDDATE_FACT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_ENDDATE_FACT\")\n",
    "\n",
    "f_AMT_CREDIT_MAX_OVERDUE = Feature(name=\"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_MAX_OVERDUE\")\n",
    "\n",
    "f_CNT_CREDIT_PROLONG = Feature(name=\"f_CNT_CREDIT_PROLONG\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CNT_CREDIT_PROLONG\")\n",
    "\n",
    "f_AMT_CREDIT_SUM = Feature(name=\"f_AMT_CREDIT_SUM\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_DEBT = Feature(name=\"f_AMT_CREDIT_SUM_DEBT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_DEBT\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_LIMIT = Feature(name=\"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_LIMIT\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_OVERDUE = Feature(name=\"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_OVERDUE\")\n",
    "\n",
    "f_CREDIT_TYPE = Feature(name=\"f_CREDIT_TYPE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_TYPE\")\n",
    "\n",
    "f_DAYS_CREDIT_UPDATE = Feature(name=\"f_DAYS_CREDIT_UPDATE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT_UPDATE\")\n",
    "\n",
    "f_AMT_ANNUITY = Feature(name=\"f_AMT_ANNUITY\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_ANNUITY\")\n",
    "\n",
    "\n",
    "\n",
    "f_NUM_CREDIT_COUNT = Feature(name=\"f_NUM_CREDIT_COUNT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"NUM_CREDIT_COUNT\")\n",
    "\n",
    "f_DEBT_CREDIT_RATIO = Feature(name=\"f_DEBT_CREDIT_RATIO\",\n",
    "                        key=key_SK_ID_BUREAU,\n",
    "                        feature_type=STRING,\n",
    "                        transform=\"DEBT_CREDIT_RATIO\")\n",
    "\n",
    "\n",
    "features_bureau_source_core=[\n",
    "  f_SK_ID_CURR,\n",
    "  f_SK_ID_BUREAU,\n",
    "  f_CREDIT_ACTIVE,\n",
    "  f_CREDIT_CURRENCY,\n",
    "  f_DAYS_CREDIT,\n",
    "  f_CREDIT_DAY_OVERDUE,\n",
    "  f_DAYS_CREDIT_ENDDATE,\n",
    "  f_DAYS_ENDDATE_FACT,\n",
    "  f_AMT_CREDIT_MAX_OVERDUE,\n",
    "  f_CNT_CREDIT_PROLONG,\n",
    "  f_AMT_CREDIT_SUM,\n",
    "  f_AMT_CREDIT_SUM_DEBT,\n",
    "  f_AMT_CREDIT_SUM_LIMIT,\n",
    "  f_AMT_CREDIT_SUM_OVERDUE,\n",
    "  f_CREDIT_TYPE,\n",
    "  f_DAYS_CREDIT_UPDATE,\n",
    "  f_AMT_ANNUITY,\n",
    "\n",
    "  f_NUM_CREDIT_COUNT,\n",
    "  f_DEBT_CREDIT_RATIO,\n",
    "  ]\n",
    "\n",
    "anchor_bureau_source_core = FeatureAnchor(name=\"anchor_bureau_source_core\",\n",
    "                                source=bureau_source_core, #INPUT_CONTEXT,\n",
    "                                features=features_bureau_source_core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for aggregated features of BUREAU\n",
    "bureau_source_agg = HdfsSource(name=\"bureauSourceAgg\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau.csv\",\n",
    "                          preprocessing=add_tran_date_column,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for aggregated features\n",
    "bureau_balance_source_core = HdfsSource(name=\"bureauBalanceSourceCore\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau_balance.csv\",\n",
    "                          preprocessing=bureau_balance_preprocessing,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "\n",
    "# pass through columns of BUREAU_BALANCE datasource CSV\n",
    "# columns of BUREAU_BALANCE\n",
    "# SK_ID_BUREAU,MONTHS_BALANCE,STATUS\n",
    "\n",
    "# f_SK_ID_BUREAU  = Feature(name=\"f_SK_ID_BUREAU\",\n",
    "#                         key=key_SK_ID_BUREAU, \n",
    "#                         feature_type=INT32, \n",
    "#                         transform=\"SK_ID_BUREAU\")\n",
    "\n",
    "f_MONTHS_BALANCE  = Feature(name=\"f_MONTHS_BALANCE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"MONTHS_BALANCE\")\n",
    "\n",
    "f_STATUS  = Feature(name=\"f_STATUS\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING,\n",
    "                        transform=\"STATUS\")\n",
    "                        # transform=\"if_else(or(STATUS == 'X', STATUS == 'C'), 0, cast_float(STATUS))\")\n",
    "                        # transform=\"cast_float(STATUS)\")\n",
    "f_CREDIT_STATUS_EMA_AVG  = Feature(name=\"f_CREDIT_STATUS_EMA_AVG\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING,\n",
    "                        transform=\"CREDIT_STATUS_EMA_AVG\")\n",
    "                        \n",
    "features_bureau_balance_source_core=[\n",
    "  # f_SK_ID_BUREAU,\n",
    "  f_MONTHS_BALANCE,\n",
    "  f_STATUS,\n",
    "  f_CREDIT_STATUS_EMA_AVG\n",
    "  ]\n",
    "\n",
    "anchor_bureau_balance_source_core = FeatureAnchor(name=\"anchor_bureau_balance_source_core\",\n",
    "                                source=bureau_balance_source_core,\n",
    "                                features=features_bureau_balance_source_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived features BUREAU_BALANCE\n",
    "\n",
    "# source for aggregated features of BUREAU_BALANCE\n",
    "# constant event_timestamp_column (1648655667 - \"epoch\")\n",
    "bureau_balance_source_agg = HdfsSource(name=\"bureauBalanceSourceAgg\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau_balance.csv\",\n",
    "                          # event_timestamp_column=\"1648655667\",\n",
    "                          # timestamp_format=\"epoch\"\n",
    "                          preprocessing=add_tran_date_column,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "\n",
    "# f_CREDIT_STATUS_EMA_AVG = Feature(name=\"f_CREDIT_STATUS_EMA_AVG\",\n",
    "#                         key=key_SK_ID_BUREAU,\n",
    "#                         feature_type=STRING,\n",
    "#                         transform=WindowAggTransformation(\n",
    "#                             agg_expr=\"if_else(or(STATUS == 'X', STATUS == 'C'), 0, STATUS)\",\n",
    "#                             agg_func=\"AVG\",\n",
    "#                             window=\"1d\"))\n",
    "\n",
    "# f_CREDIT_STATUS_EMA_AVG = Feature(name=\"f_CREDIT_STATUS_EMA_AVG\",\n",
    "#                         key=key_SK_ID_BUREAU,\n",
    "#                         feature_type=STRING,\n",
    "#                         transform=\"if_else(or(STATUS == 'X', STATUS == 'C'), 0, 1)\")\n",
    "\n",
    "\n",
    "f_CREDIT_STATUS_EMA_AVG_DER = DerivedFeature(name = \"f_CREDIT_STATUS_EMA_AVG_DER\",\n",
    "                                   feature_type = STRING,\n",
    "                                   key=[key_SK_ID_BUREAU],\n",
    "                                   input_features = [f_CREDIT_STATUS_EMA_AVG],\n",
    "                                  #  transform = \"if_else(f_STATUS == 'C', '0', if_else(f_STATUS == 'X', '0', f_STATUS))\")\n",
    "                                   transform = \"f_CREDIT_STATUS_EMA_AVG\")\n",
    "\n",
    "features_bureau_balance_source_agg = [\n",
    "  f_CREDIT_STATUS_EMA_AVG_DER\n",
    "  ]\n",
    "\n",
    "anchor_bureau_balance_source_agg = FeatureAnchor(name=\"anchor_bureau_balance_source_agg\",\n",
    "                                source=bureau_balance_source_agg,\n",
    "                                features=features_bureau_balance_source_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we need to build those features so that it can be consumed later. Note that we have to build both the \"anchor\" and the \"derived\" features (which is not anchored to a source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[\n",
    "        anchor_bureau_source_core,\n",
    "        anchor_bureau_balance_source_core,\n",
    "        # anchor_bureau_balance_source_agg\n",
    "        ], \n",
    "    derived_feature_list=[\n",
    "        f_CREDIT_STATUS_EMA_AVG_DER\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data using point-in-time correct feature join\n",
    "\n",
    "A training dataset usually contains entity id columns, multiple feature columns, event timestamp column and label/target column. \n",
    "\n",
    "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n",
    "what features and how these features should be joined to the observation data. The feature join config file mainly contains: \n",
    "\n",
    "1. The path of a dataset as the 'spine' for the to-be-created training dataset. We call this input 'spine' dataset the 'observation'\n",
    "   dataset. Typically, each row of the observation data contains: \n",
    "   a) Column(s) representing entity id(s), which will be used as the join key to look up(join) feature value. \n",
    "   b) A column representing the event time of the row. By default, Feathr will make sure the feature values joined have\n",
    "   a timestamp earlier than it, ensuring no data leakage in the resulting training dataset. \n",
    "   c) Other columns will be simply pass through onto the output training dataset.\n",
    "2. The key fields from the observation data, which are used to joined with the feature data.\n",
    "3. List of feature names to be joined with the observation data. The features must be defined in the feature\n",
    "   definition configs.\n",
    "4. The time information of the observation data used to compare with the feature's timestamp during the join.\n",
    "\n",
    "Create training dataset via:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 14:22:17.137 | INFO     | feathr._synapse_submission:upload_or_get_cloud_path:62 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feathr_pyspark_driver.py to cloud..\n",
      "2022-07-07 14:22:17.138 | INFO     | feathr._synapse_submission:upload_file:360 - Uploading file feathr_pyspark_driver.py\n",
      "2022-07-07 14:22:20.124 | INFO     | feathr._synapse_submission:upload_file:366 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feathr_pyspark_driver.py is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feathr_pyspark_driver.py\n",
      "2022-07-07 14:22:20.125 | INFO     | feathr._synapse_submission:upload_or_get_cloud_path:65 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feathr_pyspark_driver.py is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feathr_pyspark_driver.py\n",
      "2022-07-07 14:22:20.161 | INFO     | feathr._synapse_submission:upload_or_get_cloud_path:62 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_join_conf/feature_join.conf to cloud..\n",
      "2022-07-07 14:22:20.162 | INFO     | feathr._synapse_submission:upload_file:360 - Uploading file feature_join.conf\n",
      "2022-07-07 14:22:21.289 | INFO     | feathr._synapse_submission:upload_file:366 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_join_conf/feature_join.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feature_join.conf\n",
      "2022-07-07 14:22:21.290 | INFO     | feathr._synapse_submission:upload_or_get_cloud_path:65 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_join_conf/feature_join.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feature_join.conf\n",
      "2022-07-07 14:22:21.291 | INFO     | feathr._synapse_submission:upload_or_get_cloud_path:62 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_conf/ to cloud..\n",
      "2022-07-07 14:22:21.292 | INFO     | feathr._synapse_submission:upload_file_to_workdir:348 - Uploading folder /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_conf/\n",
      "2022-07-07 14:22:21.295 | INFO     | feathr._synapse_submission:upload_file:360 - Uploading file auto_generated_request_features.conf\n",
      "2022-07-07 14:22:22.518 | INFO     | feathr._synapse_submission:upload_file:366 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_conf/auto_generated_request_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_request_features.conf\n",
      "2022-07-07 14:22:22.519 | INFO     | feathr._synapse_submission:upload_file:360 - Uploading file auto_generated_anchored_features.conf\n",
      "2022-07-07 14:22:23.746 | INFO     | feathr._synapse_submission:upload_file:366 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_conf/auto_generated_anchored_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_anchored_features.conf\n",
      "2022-07-07 14:22:23.747 | INFO     | feathr._synapse_submission:upload_file:360 - Uploading file auto_generated_derived_features.conf\n",
      "2022-07-07 14:22:24.874 | INFO     | feathr._synapse_submission:upload_file:366 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_conf/auto_generated_derived_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_derived_features.conf\n",
      "2022-07-07 14:22:24.876 | INFO     | feathr._synapse_submission:upload_or_get_cloud_path:65 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpgfg2zoq5/feature_conf/ is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_request_features.conf,abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_anchored_features.conf,abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_derived_features.conf\n",
      "2022-07-07 14:22:24.877 | INFO     | feathr._envvariableutil:get_environment_variable:66 - S3_ACCESS_KEY is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.877 | INFO     | feathr._envvariableutil:get_environment_variable:66 - S3_SECRET_KEY is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.878 | INFO     | feathr._envvariableutil:get_environment_variable:66 - ADLS_ACCOUNT is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.879 | INFO     | feathr._envvariableutil:get_environment_variable:66 - ADLS_KEY is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.880 | INFO     | feathr._envvariableutil:get_environment_variable:66 - BLOB_ACCOUNT is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.880 | INFO     | feathr._envvariableutil:get_environment_variable:66 - BLOB_KEY is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.881 | INFO     | feathr._envvariableutil:get_environment_variable:66 - JDBC_TABLE is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.882 | INFO     | feathr._envvariableutil:get_environment_variable:66 - JDBC_USER is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.882 | INFO     | feathr._envvariableutil:get_environment_variable:66 - JDBC_PASSWORD is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.883 | INFO     | feathr._envvariableutil:get_environment_variable:66 - JDBC_DRIVER is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.883 | INFO     | feathr._envvariableutil:get_environment_variable:66 - JDBC_AUTH_FLAG is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.884 | INFO     | feathr._envvariableutil:get_environment_variable:66 - JDBC_TOKEN is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.911 | INFO     | feathr._envvariableutil:get_environment_variable:66 - JDBC_SF_PASSWORD is not set in the environment variables, fetching the value from Key Vault\n",
      "2022-07-07 14:22:24.911 | INFO     | feathr._synapse_submission:submit_feathr_job:105 - Uploading jar from wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar to cloud for running job: feathr_home_credit_feathr_feature_join_job\n",
      "2022-07-07 14:22:24.912 | INFO     | feathr._synapse_submission:upload_file_to_workdir:343 - Skipping file wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar as it's already in the cloud\n",
      "2022-07-07 14:22:24.912 | INFO     | feathr._synapse_submission:submit_feathr_job:108 - wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar is uploaded to wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar for running job: feathr_home_credit_feathr_feature_join_job\n",
      "2022-07-07 14:22:26.618 | INFO     | feathr._synapse_submission:submit_feathr_job:124 - See submitted job here: https://web.azuresynapse.net/en-us/monitoring/sparkapplication\n",
      "2022-07-07 14:22:26.923 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: not_started\n",
      "2022-07-07 14:22:57.241 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: not_started\n",
      "2022-07-07 14:23:27.543 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: not_started\n",
      "2022-07-07 14:23:57.853 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: not_started\n",
      "2022-07-07 14:24:28.164 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: not_started\n",
      "2022-07-07 14:24:58.443 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: starting\n",
      "2022-07-07 14:25:28.782 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: starting\n",
      "2022-07-07 14:25:59.093 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:26:29.410 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:26:59.714 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:27:30.051 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:28:00.334 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:28:30.647 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:29:00.925 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:29:31.266 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:30:01.577 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:30:31.884 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:31:02.150 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:31:32.433 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:32:02.717 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: running\n",
      "2022-07-07 14:32:33.137 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: success\n"
     ]
    }
   ],
   "source": [
    "feature_queries = [\n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            \"f_SK_ID_CURR\",\n",
    "            \"f_SK_ID_BUREAU\",\n",
    "            \"f_CREDIT_ACTIVE\",\n",
    "            \"f_CREDIT_CURRENCY\",\n",
    "            \"f_DAYS_CREDIT\",\n",
    "            \"f_CREDIT_DAY_OVERDUE\",\n",
    "            \"f_DAYS_CREDIT_ENDDATE\",\n",
    "            \"f_DAYS_ENDDATE_FACT\",\n",
    "            \"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "            \"f_CNT_CREDIT_PROLONG\",\n",
    "            \"f_AMT_CREDIT_SUM\",\n",
    "            \"f_AMT_CREDIT_SUM_DEBT\",\n",
    "            \"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "            \"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "            \"f_CREDIT_TYPE\",\n",
    "            \"f_DAYS_CREDIT_UPDATE\",\n",
    "            \"f_AMT_ANNUITY\",\n",
    "\n",
    "            # \"TRAN_DATE\"\n",
    "            \"f_NUM_CREDIT_COUNT\",\n",
    "            \"f_DEBT_CREDIT_RATIO\",\n",
    "        ], key=key_SK_ID_BUREAU),\n",
    "    \n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            # \"f_SK_ID_BUREAU\",\n",
    "            \"f_MONTHS_BALANCE\",\n",
    "            \"f_STATUS\",\n",
    "            \"f_CREDIT_STATUS_EMA_AVG\"\n",
    "        ], key=key_SK_ID_BUREAU),\n",
    "    \n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            \"f_CREDIT_STATUS_EMA_AVG_DER\"\n",
    "        ], key=key_SK_ID_BUREAU),\n",
    "]\n",
    "\n",
    "# spine dataset was created manually, it's the same as the bureau.csv \n",
    "# but with an added column named TRAN_DATE with a value of \n",
    "# datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X') (see. ./scripts/add_time_to_csv.py)\n",
    "settings = ObservationSettings(\n",
    "    observation_path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau_all_time.csv\",\n",
    "    event_timestamp_column=\"TRAN_DATE\",\n",
    "    timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    ")\n",
    "\n",
    "\n",
    "# output would be in output_bureau.avro\n",
    "client.get_offline_features(observation_settings=settings,\n",
    "                            feature_query=feature_queries,\n",
    "                            output_path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/output_bureau.avro\")\n",
    "client.wait_job_to_finish(timeout_sec=7200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the result and show the result\n",
    "\n",
    "Let's use the helper function `get_result_df` to download the result and view it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 14:32:33.751 | INFO     | feathr._synapse_submission:wait_for_completion:134 - Current Spark job status: success\n",
      "2022-07-07 14:32:34.100 | INFO     | feathr._synapse_submission:download_file:378 - Beginning reading of results from abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/output_bureau.avro\n",
      "Downloading result files: 100%|██████████| 201/201 [03:37<00:00,  1.08s/it]\n",
      "2022-07-07 14:36:13.539 | INFO     | feathr._synapse_submission:download_file:407 - Finish downloading files from abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/output_bureau.avro to ../../results/output_bureau.avro.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "def get_result_df(client: FeathrClient) -> pd.DataFrame:\n",
    "    \"\"\"Download the job result dataset from cloud as a Pandas dataframe.\"\"\"\n",
    "    res_url = client.get_job_result_uri(block=True, timeout_sec=600)\n",
    "    tmp_dir = \"../../../results/output_bureau.avro\"\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    client.feathr_spark_laucher.download_result(result_path=res_url, local_folder=tmp_dir)\n",
    "    dataframe_list = []\n",
    "    # assuming the result are in avro format\n",
    "    for file in glob.glob(os.path.join(tmp_dir, '*.avro')):\n",
    "        dataframe_list.append(pdx.read_avro(file))\n",
    "    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n",
    "    return vertical_concat_df\n",
    "\n",
    "df_res = get_result_df(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          _c0 SK_ID_CURR SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY  \\\n",
      "0       61538     298636      5000045        Active      currency 1   \n",
      "1       61777     167876      5000328        Closed      currency 1   \n",
      "2     1706086     330334      5000396        Closed      currency 1   \n",
      "3       61851     202414      5000418        Active      currency 1   \n",
      "4       61869     169025      5000439        Active      currency 1   \n",
      "...       ...        ...          ...           ...             ...   \n",
      "8449  1704166     299309      6842132        Closed      currency 1   \n",
      "8450  1704317     299929      6842323        Closed      currency 1   \n",
      "8451  1704479     422290      6842513        Closed      currency 1   \n",
      "8452  1704955     367737      6843060        Closed      currency 1   \n",
      "8453  1705111     372710      6843253        Closed      currency 1   \n",
      "\n",
      "     DAYS_CREDIT CREDIT_DAY_OVERDUE DAYS_CREDIT_ENDDATE DAYS_ENDDATE_FACT  \\\n",
      "0           -371                  0                None              None   \n",
      "1          -2502                  0             -2410.0           -1976.0   \n",
      "2          -1825                  0             -1643.0           -1643.0   \n",
      "3            -97                  0               999.0              None   \n",
      "4          -1221                  0                None              None   \n",
      "...          ...                ...                 ...               ...   \n",
      "8449       -2225                  0             -1860.0           -1860.0   \n",
      "8450       -1069                  0              -888.0            -888.0   \n",
      "8451       -2283                  0             -2068.0           -2066.0   \n",
      "8452       -1211                  0              -363.0            -363.0   \n",
      "8453       -1052                  0              -960.0            -959.0   \n",
      "\n",
      "     AMT_CREDIT_MAX_OVERDUE  ... f_DAYS_CREDIT f_CNT_CREDIT_PROLONG  \\\n",
      "0                      None  ...          -371                    0   \n",
      "1                       0.0  ...         -2502                    0   \n",
      "2                      None  ...         -1825                    0   \n",
      "3                      None  ...           -97                    0   \n",
      "4                     694.8  ...         -1221                    0   \n",
      "...                     ...  ...           ...                  ...   \n",
      "8449                   None  ...         -2225                    0   \n",
      "8450                   None  ...         -1069                    0   \n",
      "8451                    0.0  ...         -2283                    0   \n",
      "8452                   None  ...         -1211                    0   \n",
      "8453                   None  ...         -1052                    0   \n",
      "\n",
      "     f_CREDIT_ACTIVE f_NUM_CREDIT_COUNT f_CREDIT_CURRENCY    f_CREDIT_TYPE  \\\n",
      "0             Active                 13        currency 1         Mortgage   \n",
      "1             Closed                 13        currency 1  Consumer credit   \n",
      "2             Closed                 12        currency 1  Consumer credit   \n",
      "3             Active                  2        currency 1  Consumer credit   \n",
      "4             Active                  7        currency 1      Credit card   \n",
      "...              ...                ...               ...              ...   \n",
      "8449          Closed                  6        currency 1      Credit card   \n",
      "8450          Closed                  6        currency 1      Credit card   \n",
      "8451          Closed                  9        currency 1  Consumer credit   \n",
      "8452          Closed                 12        currency 1  Consumer credit   \n",
      "8453          Closed                 22        currency 1  Consumer credit   \n",
      "\n",
      "     f_MONTHS_BALANCE f_STATUS f_CREDIT_STATUS_EMA_AVG  \\\n",
      "0                None     None                    None   \n",
      "1                None     None                    None   \n",
      "2                None     None                    None   \n",
      "3                None     None                    None   \n",
      "4                None     None                    None   \n",
      "...               ...      ...                     ...   \n",
      "8449             None     None                    None   \n",
      "8450             None     None                    None   \n",
      "8451             None     None                    None   \n",
      "8452             None     None                    None   \n",
      "8453             None     None                    None   \n",
      "\n",
      "     f_CREDIT_STATUS_EMA_AVG_DER  \n",
      "0                           None  \n",
      "1                           None  \n",
      "2                           None  \n",
      "3                           None  \n",
      "4                           None  \n",
      "...                          ...  \n",
      "8449                        None  \n",
      "8450                        None  \n",
      "8451                        None  \n",
      "8452                        None  \n",
      "8453                        None  \n",
      "\n",
      "[1716428 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'SK_ID_CURR', 'SK_ID_BUREAU', 'CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY', 'TRAN_DATE', 'f_CREDIT_DAY_OVERDUE', 'f_AMT_CREDIT_MAX_OVERDUE', 'f_AMT_CREDIT_SUM_OVERDUE', 'f_AMT_CREDIT_SUM_LIMIT', 'f_DAYS_ENDDATE_FACT', 'f_DAYS_CREDIT_ENDDATE', 'f_AMT_CREDIT_SUM', 'f_SK_ID_CURR', 'f_AMT_CREDIT_SUM_DEBT', 'f_DEBT_CREDIT_RATIO', 'f_SK_ID_BUREAU', 'f_DAYS_CREDIT_UPDATE', 'f_AMT_ANNUITY', 'f_DAYS_CREDIT', 'f_CNT_CREDIT_PROLONG', 'f_CREDIT_ACTIVE', 'f_NUM_CREDIT_COUNT', 'f_CREDIT_CURRENCY', 'f_CREDIT_TYPE', 'f_MONTHS_BALANCE', 'f_STATUS', 'f_CREDIT_STATUS_EMA_AVG', 'f_CREDIT_STATUS_EMA_AVG_DER']\n",
      "      f_SK_ID_CURR f_NUM_CREDIT_COUNT f_DEBT_CREDIT_RATIO\n",
      "0           298636                 13           0.9792915\n",
      "1           167876                 13                 0.0\n",
      "2           330334                 12                 0.0\n",
      "3           202414                  2   0.960327868852459\n",
      "4           169025                  7                 0.0\n",
      "...            ...                ...                 ...\n",
      "8449        299309                  6                 0.0\n",
      "8450        299929                  6                 0.0\n",
      "8451        422290                  9                 0.0\n",
      "8452        367737                 12                 0.0\n",
      "8453        372710                 22                 0.0\n",
      "\n",
      "[1716428 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with pd.option_context('display.max_columns', 50, 'display.max_rows', 1000):\n",
    "   print(df_res.columns.values.tolist())\n",
    "   print(df_res[[\n",
    "      \"f_SK_ID_CURR\",\n",
    "      # \"f_SK_ID_BUREAU\",\n",
    "      # \"f_CREDIT_ACTIVE\",\n",
    "      # \"f_CREDIT_CURRENCY\",\n",
    "      # \"f_DAYS_CREDIT\",\n",
    "      # \"f_CREDIT_DAY_OVERDUE\",\n",
    "      # \"f_DAYS_CREDIT_ENDDATE\",\n",
    "      # \"f_DAYS_ENDDATE_FACT\",\n",
    "      # \"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "      # \"f_CNT_CREDIT_PROLONG\",\n",
    "      # \"f_AMT_CREDIT_SUM\",\n",
    "      # \"f_AMT_CREDIT_SUM_DEBT\",\n",
    "      # \"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "      # \"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "      # \"f_CREDIT_TYPE\",\n",
    "      # \"f_DAYS_CREDIT_UPDATE\",\n",
    "      # \"f_AMT_ANNUITY\",\n",
    "\n",
    "      # \"f_SK_ID_BUREAU\",\n",
    "      # \"f_MONTHS_BALANCE\",\n",
    "      # \"f_STATUS\",\n",
    "      # \"f_CREDIT_STATUS_EMA_AVG_DER\",\n",
    "\n",
    "      # # \"TRAN_DATE\",\n",
    "      # \"f_CREDIT_STATUS_EMA_AVG\",\n",
    "\n",
    "      \"f_NUM_CREDIT_COUNT\",\n",
    "      \"f_DEBT_CREDIT_RATIO\"\n",
    "   ]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "345a509dc56d979ac3261ff2283eab94d92c7e49c33244e182cdb66ed5bdbc50"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
