{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feathr Feature Store on Home Credit\n",
    "\n",
    "This notebook illustrates the use of Feature Store to create a model for home credits. It includes these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite: Install Feathr\n",
    "\n",
    "Install Feathr using pip:\n",
    "\n",
    "`pip install -U feathr pandavro scikit-learn`\n",
    "\n",
    "Or if you want to use the latest Feathr code from GitHub:\n",
    "\n",
    "`pip install -I git+https://github.com/linkedin/feathr.git#subdirectory=feathr_project pandavro scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feathr in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (0.7.1)\n",
      "Requirement already satisfied: pandavro in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (1.1.2)\n",
      "Requirement already satisfied: pyspark>=3.1.2 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (3.3.0)\n",
      "Requirement already satisfied: google>=3.0.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (4.64.0)\n",
      "Requirement already satisfied: databricks-cli in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.17.3)\n",
      "Requirement already satisfied: avro in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (1.11.1)\n",
      "Requirement already satisfied: requests in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (2.28.1)\n",
      "Requirement already satisfied: pyhocon in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.3.59)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (4.3.0)\n",
      "Requirement already satisfied: py4j in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.10.9.5)\n",
      "Requirement already satisfied: pandas in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (1.4.4)\n",
      "Requirement already satisfied: pyyaml in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (6.0)\n",
      "Requirement already satisfied: graphlib-backport in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (1.0.3)\n",
      "Requirement already satisfied: azure-synapse-spark in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.7.0)\n",
      "Requirement already satisfied: deltalake in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.6.0)\n",
      "Requirement already satisfied: pyarrow in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (9.0.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake>=12.5.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (12.6.0)\n",
      "Requirement already satisfied: google-api-python-client>=2.41.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (2.58.0)\n",
      "Requirement already satisfied: pyapacheatlas in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.13.1)\n",
      "Requirement already satisfied: python-snappy in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.6.1)\n",
      "Requirement already satisfied: confluent-kafka in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (1.9.2)\n",
      "Requirement already satisfied: azure-core<=1.22.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (1.22.1)\n",
      "Requirement already satisfied: Jinja2 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (3.1.2)\n",
      "Requirement already satisfied: Click in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (8.1.3)\n",
      "Requirement already satisfied: loguru in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (0.6.0)\n",
      "Requirement already satisfied: azure-keyvault-secrets in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (4.5.1)\n",
      "Requirement already satisfied: redis in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (4.3.4)\n",
      "Requirement already satisfied: azure-identity>=1.8.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from feathr) (1.10.0)\n",
      "Requirement already satisfied: fastavro==1.5.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from pandavro) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from pandavro) (1.23.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from azure-core<=1.22.1->feathr) (1.16.0)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.12.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from azure-identity>=1.8.0->feathr) (1.18.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from azure-identity>=1.8.0->feathr) (1.0.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from azure-identity>=1.8.0->feathr) (37.0.4)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.10.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from azure-storage-file-datalake>=12.5.0->feathr) (12.11.0)\n",
      "Requirement already satisfied: msrest>=0.6.21 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from azure-storage-file-datalake>=12.5.0->feathr) (0.6.21)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google>=3.0.0->feathr) (4.11.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (4.1.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (2.10.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (2.11.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (0.20.4)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-api-python-client>=2.41.0->feathr) (0.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from pandas->feathr) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from pandas->feathr) (2.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from requests->feathr) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from requests->feathr) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from requests->feathr) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from requests->feathr) (3.3)\n",
      "Requirement already satisfied: azure-common~=1.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from azure-keyvault-secrets->feathr) (1.1.28)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from databricks-cli->feathr) (2.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from databricks-cli->feathr) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from databricks-cli->feathr) (0.8.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from Jinja2->feathr) (2.1.1)\n",
      "Requirement already satisfied: openpyxl>=3.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from pyapacheatlas->feathr) (3.0.10)\n",
      "Requirement already satisfied: pyparsing~=2.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from pyhocon->feathr) (2.4.7)\n",
      "Requirement already satisfied: deprecated>=1.2.3 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from redis->feathr) (1.2.13)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from redis->feathr) (4.0.2)\n",
      "Requirement already satisfied: packaging>=20.4 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from redis->feathr) (21.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from cryptography>=2.5->azure-identity>=1.8.0->feathr) (1.15.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from deprecated>=1.2.3->redis->feathr) (1.14.1)\n",
      "Requirement already satisfied: protobuf<5.0.0dev,>=3.20.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=2.41.0->feathr) (4.21.5)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=2.41.0->feathr) (1.56.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=2.41.0->feathr) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=2.41.0->feathr) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=2.41.0->feathr) (0.2.8)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity>=1.8.0->feathr) (2.5.1)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from msrest>=0.6.21->azure-storage-file-datalake>=12.5.0->feathr) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from msrest>=0.6.21->azure-storage-file-datalake>=12.5.0->feathr) (1.3.1)\n",
      "Requirement already satisfied: et-xmlfile in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from openpyxl>=3.0->pyapacheatlas->feathr) (1.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from beautifulsoup4->google>=3.0.0->feathr) (2.3.2.post1)\n",
      "Requirement already satisfied: pycparser in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity>=1.8.0->feathr) (2.21)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/dasorbit/Library/CloudStorage/OneDrive-Microsoft/projects/personal/feature-store/.venv/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=2.41.0->feathr) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U feathr pandavro scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite: Configure the required environment\n",
    "\n",
    "In the first step (Provision cloud resources), you should have provisioned all the required cloud resources. If you use Feathr CLI to create a workspace, you should have a folder with a file called `feathr_config.yaml` in it with all the required configurations. Otherwise, update the configuration below.\n",
    "\n",
    "The code below will write this configuration string to a temporary location and load it to Feathr. Please still refer to [feathr_config.yaml](https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "yaml_config = \"\"\"\n",
    "# Please refer to https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml for explanations on the meaning of each field.\n",
    "api_version: 1\n",
    "project_config:\n",
    "  project_name: 'feathr_home_credit'\n",
    "  required_environment_variables:\n",
    "    - 'REDIS_PASSWORD'\n",
    "    - 'AZURE_CLIENT_ID'\n",
    "    - 'AZURE_TENANT_ID'\n",
    "    - 'AZURE_CLIENT_SECRET'\n",
    "offline_store:\n",
    "  adls:\n",
    "    adls_enabled: tru\n",
    "  wasb:\n",
    "    wasb_enabled: true\n",
    "  s3:\n",
    "    s3_enabled: false\n",
    "    s3_endpoint: 's3.amazonaws.com'\n",
    "  jdbc:\n",
    "    jdbc_enabled: false\n",
    "    jdbc_database: 'feathrtestdb'\n",
    "    jdbc_table: 'feathrtesttable'\n",
    "  snowflake:\n",
    "    url: \"dqllago-ol19457.snowflakecomputing.com\"\n",
    "    user: \"feathrintegration\"\n",
    "    role: \"ACCOUNTADMIN\"\n",
    "spark_config:\n",
    "  spark_cluster: 'azure_synapse'\n",
    "  spark_result_output_parts: '1'\n",
    "  azure_synapse:\n",
    "    dev_url: \"https://feathrhomecreditcaspark.dev.azuresynapse.net\"\n",
    "    pool_name: \"spark31\"\n",
    "    # workspace dir for storing all the required configuration files and the jar resources\n",
    "    workspace_dir: \"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/\"\n",
    "    executor_size: \"Small\"\n",
    "    executor_num: 4\n",
    "    feathr_runtime_location: wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar\n",
    "  databricks:\n",
    "    workspace_instance_url: 'https://adb-6885802458123232.12.azuredatabricks.net/'\n",
    "    workspace_token_value: ''\n",
    "    config_template: {'run_name':'','new_cluster':{'spark_version':'9.1.x-scala2.12','node_type_id':'Standard_D3_v2','num_workers':2,'spark_conf':{}},'libraries':[{'jar':''}],'spark_jar_task':{'main_class_name':'','parameters':['']}}\n",
    "    work_dir: 'dbfs:/feathr_getting_started'\n",
    "    feathr_runtime_location: wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar\n",
    "online_store:\n",
    "  redis:\n",
    "    host: 'feathrhomecreditcaredis.redis.cache.windows.net'\n",
    "    port: 6380\n",
    "    ssl_enabled: True\n",
    "feature_registry:\n",
    "  purview:\n",
    "    type_system_initialization: true\n",
    "    purview_name: 'feathrhomecreditcapurview'\n",
    "    delimiter: '__'\n",
    "\"\"\"\n",
    "tmp = tempfile.NamedTemporaryFile(mode='w', delete=False)\n",
    "with open(tmp.name, \"w\") as text_file:\n",
    "    text_file.write(yaml_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the data\n",
    "\n",
    "In this tutorial, we use Feathr Feature Store to create a model that predicts NYC Taxi fares. The dataset comes from [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). The data is as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import tempfile\n",
    "from datetime import datetime, timedelta\n",
    "from math import sqrt\n",
    "\n",
    "import pandas as pd\n",
    "import pandavro as pdx\n",
    "from feathr import FeathrClient\n",
    "from feathr import BOOLEAN, FLOAT, INT32, ValueType, STRING\n",
    "from feathr import Feature, DerivedFeature, FeatureAnchor\n",
    "from feathr import BackfillTime, MaterializationSettings\n",
    "from feathr import FeatureQuery, ObservationSettings\n",
    "from feathr import RedisSink\n",
    "from feathr import INPUT_CONTEXT, HdfsSource\n",
    "from feathr import WindowAggTransformation\n",
    "from feathr import TypedKey\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup necessary environment variables\n",
    "\n",
    "You have to setup the environment variables in order to run this sample. More environment variables can be set by referring to [feathr_config.yaml](https://github.com/linkedin/feathr/blob/main/feathr_project/feathrcli/data/feathr_user_workspace/feathr_config.yaml) and use that as the source of truth. It should also have more explanations on the meaning of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['REDIS_PASSWORD'] = ''\n",
    "os.environ['AZURE_CLIENT_ID'] = ''\n",
    "os.environ['AZURE_TENANT_ID'] = '' \n",
    "os.environ['AZURE_CLIENT_SECRET'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will initialize a feathr client:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 11:11:45.993 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n",
      "2022-09-02 11:11:46.086 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - offline_store__snowflake__snowflake_enabled not found in the config file.\n",
      "2022-09-02 11:11:46.323 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - secrets__azure_key_vault__name not found in the config file.\n",
      "2022-09-02 11:11:46.332 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - feature_registry__api_endpoint not found in the config file.\n"
     ]
    }
   ],
   "source": [
    "client = FeathrClient(config_path=tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc pre-processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import lit\n",
    "# Add pre-processing functions\n",
    "# 1. On the fly datetime field\n",
    "#   1. Create a pre-processing function to add a timefield on the\n",
    "#   2. Attach the preprocessing method in the data source\n",
    "\n",
    "\n",
    "def add_tran_date_column(df: DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(\"2021-01-01 11:34:44\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_dummy_column(df: DataFrame) -> DataFrame:   \n",
    "    df = df.withColumn(\"DUMMY\", lit(\"dummy\"))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau balance pre-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_balance_preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    from pyspark import sql\n",
    "    \n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X')))\n",
    "    \n",
    "    # convert spark data frame to panda\n",
    "    df_org =  df.toPandas()\n",
    "    df_bureauBalanceRollingCreditLoan = df_org.copy()\n",
    "    \n",
    "    df_bureauBalanceRollingCreditLoan['STATUS'] = df_bureauBalanceRollingCreditLoan['STATUS'].replace(['X','C'],'0')\n",
    "    df_bureauBalanceRollingCreditLoan['STATUS'] = pd.to_numeric(df_bureauBalanceRollingCreditLoan['STATUS'])\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.groupby(\"SK_ID_BUREAU\")['STATUS'].agg(\n",
    "        lambda x: x.ewm(span=x.shape[0], adjust=False).mean().mean()\n",
    "    )\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.reset_index(name=\"CREDIT_STATUS_EMA_AVG\")\n",
    "    df_bureauBalanceRollingCreditLoan = df_bureauBalanceRollingCreditLoan.set_index('SK_ID_BUREAU')\n",
    "    df_result = pd.merge(df_org, df_bureauBalanceRollingCreditLoan, on=\"SK_ID_BUREAU\", how=\"left\")\n",
    "    \n",
    "    # convert panda to spark dataframe\n",
    "    spark_session = sql.SparkSession.builder.appName(\"pdf to sdf\").getOrCreate()\n",
    "        \n",
    "    return spark_session.createDataFrame(df_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bureau pre-processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "def bureau_preprocessing(df: DataFrame) -> DataFrame:\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    from pyspark import sql\n",
    "    from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "    def bureauBalanceRollingCreditLoan(df):\n",
    "        df_final = df.copy()\n",
    "        df_final['STATUS'] = df_final['STATUS'].replace(['X','C'],'0')\n",
    "        df_final['STATUS'] = pd.to_numeric(df_final['STATUS'])\n",
    "        df_final = df_final.groupby(\"SK_ID_BUREAU\")['STATUS'].agg(\n",
    "            lambda x: x.ewm(span=x.shape[0], adjust=False).mean().mean()\n",
    "        )\n",
    "        df_final = df_final.reset_index(name=\"CREDIT_STATUS_EMA_AVG\")\n",
    "        df_final = df_final.set_index('SK_ID_BUREAU')\n",
    "        return df_final\n",
    "\n",
    "\n",
    "    def aggCountBureau(df):\n",
    "        agg = df.groupby(\"SK_ID_CURR\")\n",
    "        # count number of loans\n",
    "        df_final = pd.DataFrame(agg['SK_ID_CURR'].agg('count').reset_index(name='NUM_CREDIT_COUNT'))\n",
    "        # count number of loans prolonged\n",
    "        loans_prolonged = agg['CNT_CREDIT_PROLONG'].sum().reset_index(name='CREDIT_PROLONG_COUNT').set_index(\"SK_ID_CURR\")\n",
    "        df_final = df_final.join(loans_prolonged,on='SK_ID_CURR')\n",
    "        # count percentage of active loans\n",
    "        active_loans = agg['CREDIT_ACTIVE'].value_counts().reset_index(name='ACTIVE_LOANS_COUNT')\n",
    "        active_loans = active_loans[active_loans['CREDIT_ACTIVE'] == 'Active'][['SK_ID_CURR','ACTIVE_LOANS_COUNT']].set_index(\"SK_ID_CURR\")\n",
    "        df_final = df_final.join(active_loans,on='SK_ID_CURR')\n",
    "        df_final['ACTIVE_LOANS_PERCENT'] = df_final['ACTIVE_LOANS_COUNT']/df_final['NUM_CREDIT_COUNT']\n",
    "        df_final.drop([\"ACTIVE_LOANS_COUNT\"], axis=1, inplace=True)\n",
    "        df_final['ACTIVE_LOANS_PERCENT'] = df_final['ACTIVE_LOANS_PERCENT'].fillna(0)\n",
    "        # count credit type\n",
    "        # one hot encode\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe_fit = ohe.fit_transform(df[[\"CREDIT_TYPE\"]])\n",
    "        credit_type = pd.DataFrame(ohe_fit, columns = ohe.get_feature_names([\"CREDIT_TYPE\"]))\n",
    "        credit_type.insert(loc=0, column='SK_ID_CURR', value=df['SK_ID_CURR'].values)\n",
    "        credit_type = credit_type.groupby(\"SK_ID_CURR\").sum()\n",
    "        df_final = df_final.join(credit_type, on=\"SK_ID_CURR\")\n",
    "        df_final = df_final.set_index(\"SK_ID_CURR\")\n",
    "\n",
    "        return df_final\n",
    "    \n",
    "    # Average number of days between loans\n",
    "    # Average number of overdue days of overdue loans\n",
    "    def aggAvgBureau(df):\n",
    "        # convert this column to numeric\n",
    "        df['DAYS_CREDIT'] = pd.to_numeric(df['DAYS_CREDIT'])\n",
    "        agg = df.groupby('SK_ID_CURR')\n",
    "       \n",
    "        # average of CREDIT_DAY_OVERDUE\n",
    "        final_df = agg['CREDIT_DAY_OVERDUE'].mean().reset_index(name = \"CREDIT_DAY_OVERDUE_MEAN\")\n",
    "        # average of days between credits of DAYS_CREDIT\n",
    "        days_credit_between = pd.DataFrame(df['SK_ID_CURR'])\n",
    "        \n",
    "        days_credit_between['diff'] = agg['DAYS_CREDIT'].diff().values\n",
    "        days_credit_between = days_credit_between.groupby(\"SK_ID_CURR\")['diff'].mean().reset_index(name = 'DAYS_CREDIT_BETWEEN_MEAN')\n",
    "        days_credit_between.set_index(\"SK_ID_CURR\",inplace=True)\n",
    "        final_df = final_df.join(days_credit_between, on='SK_ID_CURR')\n",
    "        final_df = final_df.set_index(\"SK_ID_CURR\")\n",
    "        return final_df\n",
    "\n",
    "    #  ratio of AMT_CREDIT_SUM_DEBT to AMT_CREDIT_SUM created\n",
    "    def debtCreditRatio(df):\n",
    "        df['AMT_CREDIT_SUM_DEBT'] = pd.to_numeric(df['AMT_CREDIT_SUM_DEBT'])\n",
    "        df['AMT_CREDIT_SUM'] = pd.to_numeric(df['AMT_CREDIT_SUM'])\n",
    "        #get debt:credit ratio\n",
    "        df['DEBT_CREDIT_RATIO'] = df['AMT_CREDIT_SUM_DEBT']/df['AMT_CREDIT_SUM']\n",
    "        df_final = df.groupby('SK_ID_CURR')['DEBT_CREDIT_RATIO'].mean().reset_index(name='DEBT_CREDIT_RATIO')\n",
    "        df_final = df_final.set_index(\"SK_ID_CURR\")\n",
    "\n",
    "        df_final = df_final[df_final.columns.intersection(['SK_ID_CURR', 'DEBT_CREDIT_RATIO'])]\n",
    "        \n",
    "        return df_final\n",
    "    \n",
    "    # add a TRAN_DATE column with a static date\n",
    "    df = df.withColumn(\"TRAN_DATE\", lit(datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X')))\n",
    "    df_org = df.toPandas()\n",
    "        \n",
    "    df_aggCountBureau = aggCountBureau(df_org)\n",
    "    df_aggAvgInstalments = aggAvgBureau(df_org)\n",
    "    df_debtCreditRatio = debtCreditRatio(df_org)\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    dfs.append(df_aggCountBureau)\n",
    "    dfs.append(df_aggAvgInstalments)\n",
    "    dfs.append(df_debtCreditRatio)\n",
    "\n",
    "    df_result = dfs.pop()\n",
    "    while dfs:\n",
    "        df_result = df_result.join(dfs.pop(),on='SK_ID_CURR')\n",
    "    \n",
    "    # results df would be merge to the original df\n",
    "    df_result = pd.merge(df_org, df_result, on=\"SK_ID_CURR\", how=\"left\")\n",
    "    # merging df with same column name would result a columnname with a suffix of `_x` and `_y`.\n",
    "    # Renaming the column name with suffix `_x` to retain the original column name\n",
    "    df_result.columns = df_result.columns.str.rstrip(\"_x\")\n",
    "\n",
    "    # convert panda to spark dataframe\n",
    "    spark_session = sql.SparkSession.builder.appName(\"pdf to sdf\").getOrCreate()\n",
    "    \n",
    "    return spark_session.createDataFrame(df_result)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Features with Feathr:\n",
    "\n",
    "### Bureau Dataset\n",
    "1. parent dataset: bureau.csv \n",
    "    1. count aggregation features created\n",
    "    1. average aggregation features created\n",
    "    1. debt:credit ratio feature created\n",
    "1. child dataset: bureau_balance.csv\n",
    "    1. rolling window credit loan status feature will be created and joined to parent dataset\n",
    "1. combinig/joining both datasets, which will be aggregated in line with primary key (\"SK_ID_CURR) of application_train (target dataframe) with the following features:    \n",
    "    1. count aggregation features created\n",
    "    1. average aggregation features created\n",
    "    1. debt:credit ratio feature created\n",
    "    1. rolling window credit loan status feature will be created and joined to parent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two datasource pointing to same csv, limitation that you could not mix\n",
    "# pass through and aggregated features. By separating, it must have different datasource (datasource name)\n",
    "\n",
    "# source for pass through features\n",
    "# \"TRAN_DATE\" column created on on the \"datasource_prepocessing\" method.\n",
    "bureau_source_core = HdfsSource(name=\"bureauSourceCore\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau.csv\",\n",
    "                          preprocessing=bureau_preprocessing,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "# key definition for bureau datasource\n",
    "key_SK_ID_BUREAU = TypedKey(key_column=\"SK_ID_BUREAU\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"SK ID Bureau\",\n",
    "                       full_name=\"bureau.SK_ID_BUREAU\")\n",
    "\n",
    "key_SK_ID_CURR = TypedKey(key_column=\"SK_ID_CURR\",\n",
    "                       key_column_type=ValueType.INT32,\n",
    "                       description=\"SK ID CURR\",\n",
    "                       full_name=\"bureau.SK_ID_CURR\")\n",
    "\n",
    "# pass through columns of BUREAU datasource CSV\n",
    "f_SK_ID_CURR = Feature(name=\"f_SK_ID_CURR\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=INT32, \n",
    "                        transform=\"SK_ID_CURR\")\n",
    "\n",
    "f_SK_ID_BUREAU  = Feature(name=\"f_SK_ID_BUREAU\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"SK_ID_BUREAU\")\n",
    "\n",
    "f_CREDIT_ACTIVE = Feature(name=\"f_CREDIT_ACTIVE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_ACTIVE\")\n",
    "\n",
    "f_CREDIT_CURRENCY = Feature(name=\"f_CREDIT_CURRENCY\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_CURRENCY\")\n",
    "\n",
    "f_DAYS_CREDIT = Feature(name=\"f_DAYS_CREDIT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT\")\n",
    "\n",
    "f_CREDIT_DAY_OVERDUE = Feature(name=\"f_CREDIT_DAY_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_DAY_OVERDUE\")\n",
    "\n",
    "f_DAYS_CREDIT_ENDDATE = Feature(name=\"f_DAYS_CREDIT_ENDDATE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT_ENDDATE\")\n",
    "\n",
    "f_DAYS_ENDDATE_FACT = Feature(name=\"f_DAYS_ENDDATE_FACT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_ENDDATE_FACT\")\n",
    "\n",
    "f_AMT_CREDIT_MAX_OVERDUE = Feature(name=\"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_MAX_OVERDUE\")\n",
    "\n",
    "f_CNT_CREDIT_PROLONG = Feature(name=\"f_CNT_CREDIT_PROLONG\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CNT_CREDIT_PROLONG\")\n",
    "\n",
    "f_AMT_CREDIT_SUM = Feature(name=\"f_AMT_CREDIT_SUM\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_DEBT = Feature(name=\"f_AMT_CREDIT_SUM_DEBT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_DEBT\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_LIMIT = Feature(name=\"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_LIMIT\")\n",
    "\n",
    "f_AMT_CREDIT_SUM_OVERDUE = Feature(name=\"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_CREDIT_SUM_OVERDUE\")\n",
    "\n",
    "f_CREDIT_TYPE = Feature(name=\"f_CREDIT_TYPE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"CREDIT_TYPE\")\n",
    "\n",
    "f_DAYS_CREDIT_UPDATE = Feature(name=\"f_DAYS_CREDIT_UPDATE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"DAYS_CREDIT_UPDATE\")\n",
    "\n",
    "f_AMT_ANNUITY = Feature(name=\"f_AMT_ANNUITY\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"AMT_ANNUITY\")\n",
    "\n",
    "\n",
    "\n",
    "f_NUM_CREDIT_COUNT = Feature(name=\"f_NUM_CREDIT_COUNT\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"NUM_CREDIT_COUNT\")\n",
    "\n",
    "f_DEBT_CREDIT_RATIO = Feature(name=\"f_DEBT_CREDIT_RATIO\",\n",
    "                        key=key_SK_ID_BUREAU,\n",
    "                        feature_type=STRING,\n",
    "                        transform=\"DEBT_CREDIT_RATIO\")\n",
    "\n",
    "\n",
    "features_bureau_source_core=[\n",
    "  f_SK_ID_CURR,\n",
    "  f_SK_ID_BUREAU,\n",
    "  f_CREDIT_ACTIVE,\n",
    "  f_CREDIT_CURRENCY,\n",
    "  f_DAYS_CREDIT,\n",
    "  f_CREDIT_DAY_OVERDUE,\n",
    "  f_DAYS_CREDIT_ENDDATE,\n",
    "  f_DAYS_ENDDATE_FACT,\n",
    "  f_AMT_CREDIT_MAX_OVERDUE,\n",
    "  f_CNT_CREDIT_PROLONG,\n",
    "  f_AMT_CREDIT_SUM,\n",
    "  f_AMT_CREDIT_SUM_DEBT,\n",
    "  f_AMT_CREDIT_SUM_LIMIT,\n",
    "  f_AMT_CREDIT_SUM_OVERDUE,\n",
    "  f_CREDIT_TYPE,\n",
    "  f_DAYS_CREDIT_UPDATE,\n",
    "  f_AMT_ANNUITY,\n",
    "\n",
    "  f_NUM_CREDIT_COUNT,\n",
    "  f_DEBT_CREDIT_RATIO,\n",
    "  ]\n",
    "\n",
    "anchor_bureau_source_core = FeatureAnchor(name=\"anchor_bureau_source_core\",\n",
    "                                source=bureau_source_core, #INPUT_CONTEXT,\n",
    "                                features=features_bureau_source_core)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for aggregated features of BUREAU\n",
    "bureau_source_agg = HdfsSource(name=\"bureauSourceAgg\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau.csv\",\n",
    "                          preprocessing=add_tran_date_column,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source for aggregated features\n",
    "bureau_balance_source_core = HdfsSource(name=\"bureauBalanceSourceCore\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau_balance.csv\",\n",
    "                          preprocessing=bureau_balance_preprocessing,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "f_MONTHS_BALANCE  = Feature(name=\"f_MONTHS_BALANCE\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING, \n",
    "                        transform=\"MONTHS_BALANCE\")\n",
    "\n",
    "f_STATUS  = Feature(name=\"f_STATUS\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING,\n",
    "                        transform=\"STATUS\")\n",
    "\n",
    "f_CREDIT_STATUS_EMA_AVG  = Feature(name=\"f_CREDIT_STATUS_EMA_AVG\",\n",
    "                        key=key_SK_ID_BUREAU, \n",
    "                        feature_type=STRING,\n",
    "                        transform=\"CREDIT_STATUS_EMA_AVG\")\n",
    "                        \n",
    "features_bureau_balance_source_core=[\n",
    "  f_MONTHS_BALANCE,\n",
    "  f_STATUS,\n",
    "  f_CREDIT_STATUS_EMA_AVG\n",
    "  ]\n",
    "\n",
    "anchor_bureau_balance_source_core = FeatureAnchor(name=\"anchor_bureau_balance_source_core\",\n",
    "                                source=bureau_balance_source_core,\n",
    "                                features=features_bureau_balance_source_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived features BUREAU_BALANCE\n",
    "\n",
    "# source for aggregated features of BUREAU_BALANCE\n",
    "# constant event_timestamp_column (1648655667 - \"epoch\")\n",
    "bureau_balance_source_agg = HdfsSource(name=\"bureauBalanceSourceAgg\",\n",
    "                          path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau_balance.csv\",\n",
    "                          preprocessing=add_tran_date_column,\n",
    "                          event_timestamp_column=\"TRAN_DATE\",\n",
    "                          timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    "                          )\n",
    "\n",
    "f_CREDIT_STATUS_EMA_AVG_DER = DerivedFeature(name = \"f_CREDIT_STATUS_EMA_AVG_DER\",\n",
    "                                   feature_type = STRING,\n",
    "                                   key=[key_SK_ID_BUREAU],\n",
    "                                   input_features = [f_CREDIT_STATUS_EMA_AVG],\n",
    "                                   transform = \"f_CREDIT_STATUS_EMA_AVG\")\n",
    "\n",
    "features_bureau_balance_source_agg = [\n",
    "  f_CREDIT_STATUS_EMA_AVG_DER\n",
    "  ]\n",
    "\n",
    "anchor_bureau_balance_source_agg = FeatureAnchor(name=\"anchor_bureau_balance_source_agg\",\n",
    "                                source=bureau_balance_source_agg,\n",
    "                                features=features_bureau_balance_source_agg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we need to build those features so that it can be consumed later. Note that we have to build both the \"anchor\" and the \"derived\" features (which is not anchored to a source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.build_features(\n",
    "    anchor_list=[\n",
    "        anchor_bureau_source_core,\n",
    "        anchor_bureau_balance_source_core,\n",
    "        ], \n",
    "    derived_feature_list=[\n",
    "        f_CREDIT_STATUS_EMA_AVG_DER\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data using point-in-time correct feature join\n",
    "\n",
    "A training dataset usually contains entity id columns, multiple feature columns, event timestamp column and label/target column. \n",
    "\n",
    "To create a training dataset using Feathr, one needs to provide a feature join configuration file to specify\n",
    "what features and how these features should be joined to the observation data. The feature join config file mainly contains: \n",
    "\n",
    "1. The path of a dataset as the 'spine' for the to-be-created training dataset. We call this input 'spine' dataset the 'observation'\n",
    "   dataset. Typically, each row of the observation data contains: \n",
    "   a) Column(s) representing entity id(s), which will be used as the join key to look up(join) feature value. \n",
    "   b) A column representing the event time of the row. By default, Feathr will make sure the feature values joined have\n",
    "   a timestamp earlier than it, ensuring no data leakage in the resulting training dataset. \n",
    "   c) Other columns will be simply pass through onto the output training dataset.\n",
    "2. The key fields from the observation data, which are used to joined with the feature data.\n",
    "3. List of feature names to be joined with the observation data. The features must be defined in the feature\n",
    "   definition configs.\n",
    "4. The time information of the observation data used to compare with the feature's timestamp during the join.\n",
    "\n",
    "Create training dataset via:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 11:11:47.504 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:66 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feathr_pyspark_driver.py to cloud..\n",
      "2022-09-02 11:11:47.505 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file feathr_pyspark_driver.py\n",
      "2022-09-02 11:11:49.967 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feathr_pyspark_driver.py is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feathr_pyspark_driver.py\n",
      "2022-09-02 11:11:49.968 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:70 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feathr_pyspark_driver.py is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feathr_pyspark_driver.py\n",
      "2022-09-02 11:11:50.010 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:66 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_join_conf/feature_join.conf to cloud..\n",
      "2022-09-02 11:11:50.010 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file feature_join.conf\n",
      "2022-09-02 11:11:51.056 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_join_conf/feature_join.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feature_join.conf\n",
      "2022-09-02 11:11:51.057 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:70 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_join_conf/feature_join.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feature_join.conf\n",
      "2022-09-02 11:11:51.058 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:66 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/ to cloud..\n",
      "2022-09-02 11:11:51.058 | INFO     | feathr.spark_provider._synapse_submission:upload_file_to_workdir:397 - Uploading folder /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/\n",
      "2022-09-02 11:11:51.060 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file auto_generated_request_features.conf\n",
      "2022-09-02 11:11:52.114 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/auto_generated_request_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_request_features.conf\n",
      "2022-09-02 11:11:52.115 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file auto_generated_anchored_features.conf\n",
      "2022-09-02 11:11:53.154 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/auto_generated_anchored_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_anchored_features.conf\n",
      "2022-09-02 11:11:53.155 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file auto_generated_derived_features.conf\n",
      "2022-09-02 11:11:54.195 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/auto_generated_derived_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_derived_features.conf\n",
      "2022-09-02 11:11:54.196 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:70 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/ is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_request_features.conf,abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_anchored_features.conf,abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_derived_features.conf\n",
      "2022-09-02 11:11:54.197 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_ACCOUNT is not set in the environment variables.\n",
      "2022-09-02 11:11:54.198 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_KEY is not set in the environment variables.\n",
      "2022-09-02 11:11:54.198 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-02 11:11:54.199 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-02 11:11:54.199 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:143 - Uploading jar from wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar to cloud for running job: feathr_home_credit_feathr_feature_join_job\n",
      "2022-09-02 11:11:54.200 | INFO     | feathr.spark_provider._synapse_submission:upload_file_to_workdir:392 - Skip uploading file wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar as it's already in the cloud\n",
      "2022-09-02 11:11:54.200 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:146 - wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar is uploaded to wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar for running job: feathr_home_credit_feathr_feature_join_job\n",
      "2022-09-02 11:11:55.897 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:166 - See submitted job here: https://web.azuresynapse.net/en-us/monitoring/sparkapplication\n",
      "2022-09-02 11:11:56.175 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: not_started\n",
      "2022-09-02 11:12:26.441 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: not_started\n",
      "2022-09-02 11:12:56.737 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: not_started\n",
      "2022-09-02 11:13:27.071 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: not_started\n",
      "2022-09-02 11:13:57.374 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: not_started\n",
      "2022-09-02 11:14:27.651 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: not_started\n",
      "2022-09-02 11:14:57.936 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: starting\n",
      "2022-09-02 11:15:28.206 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: starting\n",
      "2022-09-02 11:15:58.516 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:16:28.798 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:16:59.134 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:17:29.430 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:17:59.732 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:18:30.017 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:19:00.294 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:19:30.576 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:20:00.932 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:20:31.207 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:21:01.484 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:21:31.785 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:22:02.158 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:22:32.429 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 11:23:02.749 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: success\n"
     ]
    }
   ],
   "source": [
    "feature_queries = [\n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            \"f_SK_ID_CURR\",\n",
    "            \"f_SK_ID_BUREAU\",\n",
    "            \"f_CREDIT_ACTIVE\",\n",
    "            \"f_CREDIT_CURRENCY\",\n",
    "            \"f_DAYS_CREDIT\",\n",
    "            \"f_CREDIT_DAY_OVERDUE\",\n",
    "            \"f_DAYS_CREDIT_ENDDATE\",\n",
    "            \"f_DAYS_ENDDATE_FACT\",\n",
    "            \"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "            \"f_CNT_CREDIT_PROLONG\",\n",
    "            \"f_AMT_CREDIT_SUM\",\n",
    "            \"f_AMT_CREDIT_SUM_DEBT\",\n",
    "            \"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "            \"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "            \"f_CREDIT_TYPE\",\n",
    "            \"f_DAYS_CREDIT_UPDATE\",\n",
    "            \"f_AMT_ANNUITY\",\n",
    "\n",
    "            \"f_NUM_CREDIT_COUNT\",\n",
    "            \"f_DEBT_CREDIT_RATIO\",\n",
    "        ], key=key_SK_ID_BUREAU),\n",
    "    \n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            \"f_MONTHS_BALANCE\",\n",
    "            \"f_STATUS\",\n",
    "            \"f_CREDIT_STATUS_EMA_AVG\"\n",
    "        ], key=key_SK_ID_BUREAU),\n",
    "    \n",
    "    FeatureQuery(\n",
    "        feature_list=[\n",
    "            \"f_CREDIT_STATUS_EMA_AVG_DER\"\n",
    "        ], key=key_SK_ID_BUREAU),\n",
    "]\n",
    "\n",
    "# spine dataset was created manually, it's the same as the bureau.csv \n",
    "# but with an added column named TRAN_DATE with a value of \n",
    "# datetime.datetime(2021,1,1,11,34,44).strftime('%Y-%m-%d %X') (see. ./scripts/add_time_to_csv.py)\n",
    "settings = ObservationSettings(\n",
    "    observation_path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/bureau_all_time.csv\",\n",
    "    event_timestamp_column=\"TRAN_DATE\",\n",
    "    timestamp_format=\"yyyy-MM-dd HH:mm:ss\"\n",
    ")\n",
    "\n",
    "\n",
    "# output would be in output_bureau.avro\n",
    "client.get_offline_features(observation_settings=settings,\n",
    "                            feature_query=feature_queries,\n",
    "                            output_path=\"abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/output_bureau.avro\")\n",
    "client.wait_job_to_finish(timeout_sec=7200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the result and show the result\n",
    "\n",
    "Let's use the helper function `get_result_df` to download the result and view it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 11:40:43.272 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: success\n",
      "2022-09-02 11:40:43.525 | INFO     | feathr.spark_provider._synapse_submission:download_file:427 - Beginning reading of results from abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/output_bureau.avro\n",
      "Downloading result files: 100%|| 201/201 [03:51<00:00,  1.15s/it]\n",
      "2022-09-02 11:44:37.123 | INFO     | feathr.spark_provider._synapse_submission:download_file:456 - Finish downloading files from abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/home_credit_data/output_bureau.avro to ../../../results/output_bureau.avro.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "def get_result_df(client: FeathrClient) -> pd.DataFrame:\n",
    "    \"\"\"Download the job result dataset from cloud as a Pandas dataframe.\"\"\"\n",
    "    res_url = client.get_job_result_uri(block=True, timeout_sec=600)\n",
    "    tmp_dir = \"../output_bureau.avro\"\n",
    "    shutil.rmtree(tmp_dir, ignore_errors=True)\n",
    "    client.feathr_spark_launcher.download_result(result_path=res_url, local_folder=tmp_dir)\n",
    "    dataframe_list = []\n",
    "    # assuming the result are in avro format\n",
    "    for file in glob.glob(os.path.join(tmp_dir, '*.avro')):\n",
    "        dataframe_list.append(pdx.read_avro(file))\n",
    "    vertical_concat_df = pd.concat(dataframe_list, axis=0)\n",
    "    return vertical_concat_df\n",
    "\n",
    "df_res = get_result_df(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          _c0 SK_ID_CURR SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY  \\\n",
      "0       61540     298636      5000047        Closed      currency 1   \n",
      "1       61864     136548      5000433        Active      currency 1   \n",
      "2     1706436     162526      5000779        Closed      currency 1   \n",
      "3     1706446     426444      5000855        Closed      currency 1   \n",
      "4       62401     221038      5001087        Active      currency 1   \n",
      "...       ...        ...          ...           ...             ...   \n",
      "8562  1704006     197548      6841943        Closed      currency 1   \n",
      "8563  1173761     210960      6842052        Active      currency 1   \n",
      "8564  1121088     187591      6842344        Closed      currency 1   \n",
      "8565  1173439     242758      6843074        Closed      currency 1   \n",
      "8566  1705085     130212      6843220        Active      currency 1   \n",
      "\n",
      "     DAYS_CREDIT CREDIT_DAY_OVERDUE DAYS_CREDIT_ENDDATE DAYS_ENDDATE_FACT  \\\n",
      "0          -1785                  0              -495.0            -495.0   \n",
      "1           -511                  0                None              None   \n",
      "2           -624                  0                None            -455.0   \n",
      "3           -538                  0              -538.0            -538.0   \n",
      "4            -33                  0               148.0              None   \n",
      "...          ...                ...                 ...               ...   \n",
      "8562        -992                  0              -825.0            -811.0   \n",
      "8563        -545                  0              -331.0              None   \n",
      "8564       -2732                  0                None           -2366.0   \n",
      "8565        -728                  0              -421.0            -421.0   \n",
      "8566        -158                  0              1668.0              None   \n",
      "\n",
      "     AMT_CREDIT_MAX_OVERDUE  ... f_SK_ID_BUREAU f_DAYS_CREDIT_UPDATE  \\\n",
      "0                       0.0  ...        5000047                 -490   \n",
      "1                       0.0  ...        5000433                  -18   \n",
      "2                      None  ...        5000779                 -454   \n",
      "3                       0.0  ...        5000855                 -538   \n",
      "4                      None  ...        5001087                  -23   \n",
      "...                     ...  ...            ...                  ...   \n",
      "8562                    0.0  ...        6841943                 -526   \n",
      "8563                    0.0  ...        6842052                 -537   \n",
      "8564                    0.0  ...        6842344                -2366   \n",
      "8565                    0.0  ...        6843074                 -421   \n",
      "8566                   None  ...        6843220                  -30   \n",
      "\n",
      "     f_AMT_ANNUITY f_DAYS_CREDIT f_CNT_CREDIT_PROLONG f_CREDIT_ACTIVE  \\\n",
      "0             None         -1785                    2          Closed   \n",
      "1             None          -511                    0          Active   \n",
      "2             None          -624                    0          Closed   \n",
      "3             None          -538                    0          Closed   \n",
      "4             None           -33                    0          Active   \n",
      "...            ...           ...                  ...             ...   \n",
      "8562     46476.495          -992                    0          Closed   \n",
      "8563          None          -545                    0          Active   \n",
      "8564          None         -2732                    0          Closed   \n",
      "8565          None          -728                    0          Closed   \n",
      "8566          None          -158                    0          Active   \n",
      "\n",
      "     f_NUM_CREDIT_COUNT f_CREDIT_CURRENCY    f_CREDIT_TYPE  \\\n",
      "0                    13        currency 1      Credit card   \n",
      "1                     2        currency 1      Credit card   \n",
      "2                     7        currency 1  Consumer credit   \n",
      "3                    19        currency 1      Credit card   \n",
      "4                     3        currency 1  Consumer credit   \n",
      "...                 ...               ...              ...   \n",
      "8562                 25        currency 1  Consumer credit   \n",
      "8563                  1        currency 1  Consumer credit   \n",
      "8564                 15        currency 1  Consumer credit   \n",
      "8565                  5        currency 1  Consumer credit   \n",
      "8566                  8        currency 1  Consumer credit   \n",
      "\n",
      "     f_CREDIT_STATUS_EMA_AVG_DER  \n",
      "0                           None  \n",
      "1                           None  \n",
      "2                           None  \n",
      "3                           None  \n",
      "4                           None  \n",
      "...                          ...  \n",
      "8562                         0.0  \n",
      "8563                        None  \n",
      "8564                        None  \n",
      "8565                        None  \n",
      "8566                        None  \n",
      "\n",
      "[1716428 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', 'SK_ID_CURR', 'SK_ID_BUREAU', 'CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'DAYS_CREDIT', 'CREDIT_DAY_OVERDUE', 'DAYS_CREDIT_ENDDATE', 'DAYS_ENDDATE_FACT', 'AMT_CREDIT_MAX_OVERDUE', 'CNT_CREDIT_PROLONG', 'AMT_CREDIT_SUM', 'AMT_CREDIT_SUM_DEBT', 'AMT_CREDIT_SUM_LIMIT', 'AMT_CREDIT_SUM_OVERDUE', 'CREDIT_TYPE', 'DAYS_CREDIT_UPDATE', 'AMT_ANNUITY', 'TRAN_DATE', 'f_MONTHS_BALANCE', 'f_STATUS', 'f_CREDIT_STATUS_EMA_AVG', 'f_CREDIT_DAY_OVERDUE', 'f_AMT_CREDIT_MAX_OVERDUE', 'f_AMT_CREDIT_SUM_OVERDUE', 'f_AMT_CREDIT_SUM_LIMIT', 'f_DAYS_ENDDATE_FACT', 'f_DAYS_CREDIT_ENDDATE', 'f_AMT_CREDIT_SUM', 'f_SK_ID_CURR', 'f_AMT_CREDIT_SUM_DEBT', 'f_DEBT_CREDIT_RATIO', 'f_SK_ID_BUREAU', 'f_DAYS_CREDIT_UPDATE', 'f_AMT_ANNUITY', 'f_DAYS_CREDIT', 'f_CNT_CREDIT_PROLONG', 'f_CREDIT_ACTIVE', 'f_NUM_CREDIT_COUNT', 'f_CREDIT_CURRENCY', 'f_CREDIT_TYPE', 'f_CREDIT_STATUS_EMA_AVG_DER']\n",
      "      f_SK_ID_CURR f_SK_ID_BUREAU f_NUM_CREDIT_COUNT f_DEBT_CREDIT_RATIO\n",
      "0           298636        5000047                 13                None\n",
      "1           136548        5000433                  2            0.970875\n",
      "2           162526        5000779                  7                 0.0\n",
      "3           426444        5000855                 19                None\n",
      "4           221038        5001087                  3                 1.0\n",
      "...            ...            ...                ...                 ...\n",
      "8562        197548        6841943                 25                 0.0\n",
      "8563        210960        6842052                  1                 0.0\n",
      "8564        187591        6842344                 15                 0.0\n",
      "8565        242758        6843074                  5                 0.0\n",
      "8566        130212        6843220                  8  0.9631681957186544\n",
      "\n",
      "[1716428 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with pd.option_context('display.max_columns', 50, 'display.max_rows', 1000):\n",
    "   print(df_res.columns.values.tolist())\n",
    "   print(df_res[[\n",
    "      \"f_SK_ID_CURR\",\n",
    "      \"f_SK_ID_BUREAU\",\n",
    "      \"f_CREDIT_ACTIVE\",\n",
    "      \"f_CREDIT_CURRENCY\",\n",
    "      \"f_DAYS_CREDIT\",\n",
    "      \"f_CREDIT_DAY_OVERDUE\",\n",
    "      \"f_DAYS_CREDIT_ENDDATE\",\n",
    "      \"f_DAYS_ENDDATE_FACT\",\n",
    "      \"f_AMT_CREDIT_MAX_OVERDUE\",\n",
    "      \"f_CNT_CREDIT_PROLONG\",\n",
    "      \"f_AMT_CREDIT_SUM\",\n",
    "      \"f_AMT_CREDIT_SUM_DEBT\",\n",
    "      \"f_AMT_CREDIT_SUM_LIMIT\",\n",
    "      \"f_AMT_CREDIT_SUM_OVERDUE\",\n",
    "      \"f_CREDIT_TYPE\",\n",
    "      \"f_DAYS_CREDIT_UPDATE\",\n",
    "      \"f_AMT_ANNUITY\",\n",
    "\n",
    "      \"f_SK_ID_BUREAU\",\n",
    "      \"f_MONTHS_BALANCE\",\n",
    "      \"f_STATUS\",\n",
    "      \"f_CREDIT_STATUS_EMA_AVG_DER\",\n",
    "      \"f_CREDIT_STATUS_EMA_AVG\",\n",
    "\n",
    "      \"f_NUM_CREDIT_COUNT\",\n",
    "      \"f_DEBT_CREDIT_RATIO\"\n",
    "   ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-02 12:03:35.769 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:66 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feathr_pyspark_driver.py to cloud..\n",
      "2022-09-02 12:03:35.769 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file feathr_pyspark_driver.py\n",
      "2022-09-02 12:03:37.559 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feathr_pyspark_driver.py is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feathr_pyspark_driver.py\n",
      "2022-09-02 12:03:37.560 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:70 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feathr_pyspark_driver.py is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/feathr_pyspark_driver.py\n",
      "2022-09-02 12:03:37.561 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - KAFKA_SASL_JAAS_CONFIG is not set in the environment variables.\n",
      "2022-09-02 12:03:37.562 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:66 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_gen_conf/auto_gen_config_1589904000.0.conf to cloud..\n",
      "2022-09-02 12:03:37.562 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file auto_gen_config_1589904000.0.conf\n",
      "2022-09-02 12:03:38.578 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_gen_conf/auto_gen_config_1589904000.0.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_gen_config_1589904000.0.conf\n",
      "2022-09-02 12:03:38.579 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:70 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_gen_conf/auto_gen_config_1589904000.0.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_gen_config_1589904000.0.conf\n",
      "2022-09-02 12:03:38.580 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:66 - Uploading /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/ to cloud..\n",
      "2022-09-02 12:03:38.581 | INFO     | feathr.spark_provider._synapse_submission:upload_file_to_workdir:397 - Uploading folder /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/\n",
      "2022-09-02 12:03:38.583 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file auto_generated_request_features.conf\n",
      "2022-09-02 12:03:39.606 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/auto_generated_request_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_request_features.conf\n",
      "2022-09-02 12:03:39.607 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file auto_generated_anchored_features.conf\n",
      "2022-09-02 12:03:40.649 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/auto_generated_anchored_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_anchored_features.conf\n",
      "2022-09-02 12:03:40.651 | INFO     | feathr.spark_provider._synapse_submission:upload_file:409 - Uploading file auto_generated_derived_features.conf\n",
      "2022-09-02 12:03:41.670 | INFO     | feathr.spark_provider._synapse_submission:upload_file:415 - /private/var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/auto_generated_derived_features.conf is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_derived_features.conf\n",
      "2022-09-02 12:03:41.671 | INFO     | feathr.spark_provider._synapse_submission:upload_or_get_cloud_path:70 - /var/folders/gs/dbrzk90d0m3849n982_q27w40000gn/T/tmpflkjpany/feature_conf/ is uploaded to location: abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_request_features.conf,abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_anchored_features.conf,abfss://feathrhomecreditcafs@feathrhomecreditcasto.dfs.core.windows.net/auto_generated_derived_features.conf\n",
      "2022-09-02 12:03:41.672 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_ACCOUNT is not set in the environment variables.\n",
      "2022-09-02 12:03:41.673 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - ADLS_KEY is not set in the environment variables.\n",
      "2022-09-02 12:03:41.674 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_ACCOUNT is not set in the environment variables.\n",
      "2022-09-02 12:03:41.674 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - BLOB_KEY is not set in the environment variables.\n",
      "2022-09-02 12:03:41.683 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__url not found in the config file.\n",
      "2022-09-02 12:03:41.693 | INFO     | feathr.utils._envvariableutil:get_environment_variable_with_default:51 - monitoring__database__sql__user not found in the config file.\n",
      "2022-09-02 12:03:41.694 | INFO     | feathr.utils._envvariableutil:get_environment_variable:82 - MONITORING_DATABASE_SQL_PASSWORD is not set in the environment variables.\n",
      "2022-09-02 12:03:41.694 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:143 - Uploading jar from wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar to cloud for running job: feathr_home_credit_feathr_feature_materialization_job\n",
      "2022-09-02 12:03:41.695 | INFO     | feathr.spark_provider._synapse_submission:upload_file_to_workdir:392 - Skip uploading file wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar as it's already in the cloud\n",
      "2022-09-02 12:03:41.696 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:146 - wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar is uploaded to wasbs://public@azurefeathrstorage.blob.core.windows.net/feathr-assembly-LATEST.jar for running job: feathr_home_credit_feathr_feature_materialization_job\n",
      "2022-09-02 12:03:42.247 | INFO     | feathr.spark_provider._synapse_submission:submit_feathr_job:166 - See submitted job here: https://web.azuresynapse.net/en-us/monitoring/sparkapplication\n",
      "2022-09-02 12:03:42.518 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: not_started\n",
      "2022-09-02 12:04:12.882 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: starting\n",
      "2022-09-02 12:04:43.172 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:05:13.457 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:05:43.731 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:06:14.116 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:06:44.408 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:07:14.675 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:07:44.958 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:08:15.265 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:08:45.578 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:09:15.848 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:09:46.199 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:10:16.494 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:10:46.773 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: running\n",
      "2022-09-02 12:11:17.141 | INFO     | feathr.spark_provider._synapse_submission:wait_for_completion:176 - Current Spark job status: success\n"
     ]
    }
   ],
   "source": [
    "backfill_time = BackfillTime(start=datetime(2020, 5, 20), \n",
    "                             end=datetime(2020, 5, 20), \n",
    "                             step=timedelta(days=1))\n",
    "redisSink = RedisSink(table_name=\"homeCreditDemoFeature\")\n",
    "settings = MaterializationSettings(name=\"homeCreditFeatureSetting\",\n",
    "                                   backfill_time=backfill_time,\n",
    "                                   sinks=[redisSink],\n",
    "                                   feature_names=[\"f_NUM_CREDIT_COUNT\"])\n",
    "\n",
    "client.materialize_features(settings)\n",
    "client.wait_job_to_finish(timeout_sec=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_online_features('homeCreditDemoFeature', \n",
    "                           '6841943', \n",
    "                           ['f_NUM_CREDIT_COUNT'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd8e05a3b29cb52c25a673b02199ba49a1ae4abbf3dc61fdb468ec9ed0117842"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
